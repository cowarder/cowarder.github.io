<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>两阶段目标检测算法</title>
    <link href="undefined2019/11/30/two-stage-detection/"/>
    <url>2019/11/30/two-stage-detection/</url>
    
    <content type="html"><![CDATA[<p>在目标检测任务中，我们可以根据是否产生proposal将模型分为两个类别：单阶段(one-stage)目标检测算法和两阶段(two-stage)目标检测算法。</p><p>本文将整理介绍几种经典的两阶段目标检测算法(R-CNN, SPP Net, Fast R-CNN, Faster R-CNN)，主要从解决的问题、创新点、算法具体流程、仍存在的问题等方面进行一一阐述。</p><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">R-CNN</a></h3><h4 id="时间：2014"><a href="#时间：2014" class="headerlink" title="时间：2014"></a>时间：2014</h4><center><img src="/2019/11/30/two-stage-detection/RCNN.png" srcset="undefined"></center><h4 id="解决的问题："><a href="#解决的问题：" class="headerlink" title="解决的问题："></a>解决的问题：</h4><blockquote><p>如何将深度学习应用于目标检测领域？</p></blockquote><h4 id="创新点："><a href="#创新点：" class="headerlink" title="创新点："></a>创新点：</h4><blockquote><p>首次将深度学习应用到目标检测领域，打破了之前几年时间内传统方法无法获得提升的困境，大大提高了目标检测算法的性能</p></blockquote><h4 id="算法流程："><a href="#算法流程：" class="headerlink" title="算法流程："></a>算法流程：</h4><blockquote><p>1.输入图片<br>2.对输入图片利用selective search算法产生大约2k个region proposal<br>3.对所有的proposal进行warp操作固定大小，然后输入到预训练好的网络中获取feature<br>4.利用线性SVM进行分类</p></blockquote><h4 id="存在的问题："><a href="#存在的问题：" class="headerlink" title="存在的问题："></a>存在的问题：</h4><blockquote><p>1.feature extraction阶段针对每一个proposal都需要通过神经网络计算对应的feature，计算量大<br>2.region proposal、feature extraction、SVM classification都是相互独立的，逐个进行优化，很难得到一个全局最优的结果且效率低<br>3.selective search应用在原始图片上面，获取的内容都是一些底层的信息，很难提取到有效的高层语义信息，而且是在cpu上运行的，比较耗时</p></blockquote><h3 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP Net"></a><a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="noopener">SPP Net</a></h3><h4 id="时间：2014-1"><a href="#时间：2014-1" class="headerlink" title="时间：2014"></a>时间：2014</h4><center><img src="/2019/11/30/two-stage-detection/SPP.png" srcset="undefined"></center><h4 id="解决的问题：-1"><a href="#解决的问题：-1" class="headerlink" title="解决的问题："></a>解决的问题：</h4><blockquote><p>1.卷积神经网络需要输入图片具有相同的尺寸，这就需要对原始的输入图片进行crop/warp操作，这样势必会丢失许多图像信息<br>2.对每个proposal应用神经网络导致大量的计算负担</p></blockquote><center><img src="/2019/11/30/two-stage-detection/crop-wrap.png" srcset="undefined"></center><h4 id="创新点：-1"><a href="#创新点：-1" class="headerlink" title="创新点："></a>创新点：</h4><blockquote><p>1.提出了Spatial Pyramid Pooling，无论针对什么大小的proposal，都能够通过SPP将其映射为固定数目的参数向量，这样就不需要将图片resize到固定大小，也就一定程度上避免了信息的丢失</p></blockquote><blockquote><p>2.针对R-CNN存在的proposal计算特征运算量大的问题，改进为首先对原图像进行卷积操作提取feature map，然后在feature map上面找到proposal对应的的位置，这样就只需要运行一次卷积操作即可，节省了运算时间</p></blockquote><h4 id="算法流程：-1"><a href="#算法流程：-1" class="headerlink" title="算法流程："></a>算法流程：</h4><blockquote><p>1.输入图片<br>2.对输入图片利用selective search算法产生大约2k个region proposal<br>3.对整张输入图片利用卷积神经网络提取feature map<br>4.计算每个proposal在feature map上面的对应位置<br>5.利用Spatial Pyramid Network将proposal对应的feature映射为固定大小<br>6.利用线性SVM进行分类</p></blockquote><h4 id="存在的问题：-1"><a href="#存在的问题：-1" class="headerlink" title="存在的问题："></a>存在的问题：</h4><blockquote><p>1.网络模型仍然是多阶段独立的，无法实现端到端训练<br>2.SPP层之前的网络都是固定权重的，无法进行训练<br>3.proposal生成仍然采用的是selective search的方法</p></blockquote><h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><blockquote><p>SPP Net在训练的时候采用了多尺度的方法，具体流程是：在每一个epoch的时候，需要将图片resize到一个固定的尺寸(224x224)，训练网络后保存网络的参数，下一个epoch开始的时候，将图片resize为另外一个尺寸(180x180)，然后加载之前保存的网络模型参数再次进行训练。</p></blockquote><h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a><a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a></h3><h4 id="时间：2015"><a href="#时间：2015" class="headerlink" title="时间：2015"></a>时间：2015</h4><center><img src="/2019/11/30/two-stage-detection/fast-RCNN.png" srcset="undefined"></center><h4 id="解决的问题：-2"><a href="#解决的问题：-2" class="headerlink" title="解决的问题："></a>解决的问题：</h4><blockquote><p>1.SPP训练过程中对权重微调存在的困难(SPP Net中训练的时候，首先提取所有图片的region proposal，作为训练样本放入到神经网络中进行训练，每个proposal都是来自不同图片的，如果需要对SPP之前的网络进行微调，就需要保存所有的图片的feature map，对计算量和缓存的要求太大)<br>2.之前提出的目标检测算法无法实现端到端训练的过程</p></blockquote><h4 id="创新点：-2"><a href="#创新点：-2" class="headerlink" title="创新点："></a>创新点：</h4><blockquote><p>1.为了解决第一个问题，Fast R-CNN中采用了层次采样的方法。首先从所有样本中每次取N张图片，然后每张图片选取R个ROI，这里的N=2，R=128，同一图像的ROI共享计算和内存，这就意味着每次只需要计算和存储两张图片，资源消耗大大减小</p></blockquote><blockquote><p>2.提出ROI Pooling(Region Of Interest Pooling)。其实是SPP的单尺度版本，论文中提到多尺度能够获得性能提升，但是计算量成倍增加，所以综合考虑之下，单尺度版本的性能更好</p></blockquote><blockquote><p>3.除了selective search提取region proposal过程之外，其他过程(feature extraction、classification、bounding box regression)实现了端到端训练</p></blockquote><blockquote><p>4.将分类和边界框回归整合到一个网络中。不再使用SVM进行分类(通过作者的实验发现，SVM相对于softmax优势不大)，而是利用CNN网络实现了softmax classifier和bounding box regressor同时训练的过程</p></blockquote><h4 id="算法流程：-2"><a href="#算法流程：-2" class="headerlink" title="算法流程："></a>算法流程：</h4><blockquote><p>1.输入图片<br>2.对输入图片利用selective search算法产生大约2k个region proposal<br>3.将预训练网络的max pooling替换为ROI pooling，将每个region proposal映射为固定长度的特征向量<br>4.经过一系列的fc layer之后分为两个branch，一个负责预测proposal的类别，另一个负责进行bounding box regression</p></blockquote><h4 id="存在的问题：-2"><a href="#存在的问题：-2" class="headerlink" title="存在的问题："></a>存在的问题：</h4><blockquote><p>regions proposal仍然采用的是selective search的方法，时间消耗大</p></blockquote><h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a><a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a></h3><h4 id="时间：2016"><a href="#时间：2016" class="headerlink" title="时间：2016"></a>时间：2016</h4><center><img src="/2019/11/30/two-stage-detection/faster-RCNN.png" srcset="undefined"></center><h4 id="解决的问题：-3"><a href="#解决的问题：-3" class="headerlink" title="解决的问题："></a>解决的问题：</h4><blockquote><p>Fast R-CNN虽然基本实现了端到端的训练，但是selective search过程仍然是在cpu上面运行的</p></blockquote><h4 id="创新点：-3"><a href="#创新点：-3" class="headerlink" title="创新点："></a>创新点：</h4><blockquote><p>1.提出了RPN(Region Proposal Network)网络，使得原来的region proposal的过程不再是通过selective的方法，而是通过神经网络来生成，这样region proposal、feature extraction、bounding box regression、classification就融合到一个网络中国呢，真正实现端到端的训练<br>2.第一次引入了anchor的概念，详情可见上一篇文章</p></blockquote><h4 id="算法流程：-3"><a href="#算法流程：-3" class="headerlink" title="算法流程："></a>算法流程：</h4><blockquote><p>1.利用ImageNet模型初始化，训练一个RPN网络<br>2.仍然使用ImageNet模型初始化，利用步骤1产生的proposal作为输入，训练一个Fast R-CNN网络。至此，权值不共享<br>3.利用步骤2训练好的Fast R-CNN初始化一个新的RPN网络(就是前几层的网络权重设置为同样的值)，将RPN和Fast R-CNN共享的权值learning rate设置为0，不更新，只更新RPN特有的网络层，至此，两个网络就共享了所有的卷积层<br>4.仍然固定共享的卷积层，将Fast R-CNN加进来，fine-tune Fast R-CNN对应的网络层</p></blockquote><p><strong>参考资料</strong></p><p>1.<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">R-CNN</a><br>2.<a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="noopener">SPP Net</a><br>3.<a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a><br>4.<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a><br>5.<a href="https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html" target="_blank" rel="noopener">Object Detection for Dummies Part 3: R-CNN Family</a><br>6.<a href="https://www.zhihu.com/question/35887527/answer/147832196" target="_blank" rel="noopener">如何评价rcnn、fast-rcnn和faster-rcnn这一系列方法？</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>object detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>目标检测中的Anchor</title>
    <link href="undefined2019/11/21/anchor/"/>
    <url>2019/11/21/anchor/</url>
    
    <content type="html"><![CDATA[<h1 id="什么是Anchor？"><a href="#什么是Anchor？" class="headerlink" title="什么是Anchor？"></a>什么是Anchor？</h1><p>anchor机制是广泛应用于目标检测任务中的一种方法，anchor box指的是是一组预先定义好的不同长宽比的边界框，通过对这些不同长宽比的anchor进行微调，我们能够检测到对应不同尺寸的真实物体。</p><center><img src="/2019/11/21/anchor/anchor.png" srcset="undefined" width="200" height="200" alt="目标检测中的anchor">目标检测中的anchor</center><h1 id="Anchor的作用是什么？"><a href="#Anchor的作用是什么？" class="headerlink" title="Anchor的作用是什么？"></a>Anchor的作用是什么？</h1><p>考虑到图像中会出现不同尺寸(scale)，不同长宽比(aspect ratio)的物体，而我们神经网络的输出是一组同样scale的feature map和一组同样的weights，这样的情况下让其预测不同scale和aspect ratio的物体就相对来说比较困难。如图所示：</p><center><img src="/2019/11/21/anchor/multiscale.jpg" srcset="undefined">图像中的多尺度物体</center><p><a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" target="_blank" rel="noopener">Faster-RCNN</a>中首先引入anchor的概念，论文中有这样一段话:</p><blockquote><p>In contrast to prevalent methods [8], [9], [1], [2] that use pyramids of images (Figure 1, a) or pyramids of filters (Figure 1, b), we introduce novel “anchor” boxes that serve as <strong>references at multiple scales and aspect ratios</strong>. Our scheme can be thought of as a pyramid of regression references (Figure 1, c), which avoids enumerating images or filters of multiple scales or aspect ratios. This model performs well when trained and tested using single-scale images and thus benefits running speed.</p></blockquote><p>也就是说，anchor的作用在于解决图像中物体scale和aspect ratio变化范围大的问题，之前的解决方法是pyramids of images 或者 pyramids of filters，但是前者耗费的时间太多，而后者是传统的CV的方法，当时对于CNN还没有一种一种对应的pyramids of filters的方案(真正得到CNN的方法是2016年提出的FPN，pyramids of features)</p><p>anchor的方法同时还解决了另外一个问题。我们在做检测的时候，如果两个ground truth预测框之间的overlap过大，它们会落到同一个cell中，这样就会导致ground truth被过滤掉。如下图所示：</p><center><img src="/2019/11/21/anchor/overlap.jpeg" srcset="undefined">同一个cell中gt box出现overlap的情况</center><p>而如果采用的anchor方法，一个cell中不同的anchor负责去预测不同scale和aspect ratio的物体，即便是有多个框会预测到同一个cell，也不会导致ground truth丢失的情况出现。</p><p><strong>总结来说，anchor机制解决了两个问题：(1) 不同scale和aspect ratio物体预测难度大，利用anchor将scale和aspect ratio空间划分为几个子空间，降低了问题难度  (2) 解决了gt box与gt box之间overlap过大导致gt box丢失的问题。</strong></p><h1 id="Anchor的选取"><a href="#Anchor的选取" class="headerlink" title="Anchor的选取"></a>Anchor的选取</h1><p>anchor的大小是在我们进行模型训练之前就已经确定的了，那么是如何确定的呢？  </p><p>我们以Pascal VOC 2007数据集为例，采用的方法是k-means聚类，根据VOC 2007数据集中图片的长度和宽度进行聚类，这里需要注意的问题是，我们采用的是Yolov3模型，Yolov3模型采用的输入图像的size 是416*416的，因此在做聚类之前就需要将图片和对应的label做等比例的resize，然后再进行聚类，获取anchor。  </p><p>聚类代码：</p><pre><code class="python">class kMean_parse:    def __init__(self,path_txt):        self.path = path_txt        self.km = KMeans(n_clusters=9,init=&quot;k-means++&quot;,n_init=10,max_iter=3000000,tol=1e-3,random_state=0)        self._load_data()    def _load_data (self):        self.data = np.loadtxt(self.path)    def parse_data (self):        self.y_k = self.km.fit_predict(self.data)        print(self.km.cluster_centers_)    def plot_data (self):        plt.scatter(self.data[self.y_k == 0, 0], self.data[self.y_k == 0, 1], s=50, c=&quot;orange&quot;, marker=&quot;o&quot;, label=&quot;cluster 1&quot;)        plt.scatter(self.data[self.y_k == 1, 0], self.data[self.y_k == 1, 1], s=50, c=&quot;green&quot;, marker=&quot;s&quot;, label=&quot;cluster 2&quot;)        plt.scatter(self.data[self.y_k == 2, 0], self.data[self.y_k == 2, 1], s=50, c=&quot;blue&quot;, marker=&quot;^&quot;, label=&quot;cluster 3&quot;)        plt.scatter(self.data[self.y_k == 3, 0], self.data[self.y_k == 3, 1], s=50, c=&quot;gray&quot;, marker=&quot;*&quot;,label=&quot;cluster 4&quot;)        plt.scatter(self.data[self.y_k == 4, 0], self.data[self.y_k == 4, 1], s=50, c=&quot;yellow&quot;, marker=&quot;d&quot;,label=&quot;cluster 5&quot;)       # draw the centers        plt.scatter(self.km.cluster_centers_[:, 0], self.km.cluster_centers_[:, 1], s=250, marker=&quot;*&quot;, c=&quot;red&quot;, label=&quot;cluster center&quot;)        plt.legend()        plt.grid()        plt.show()</code></pre><p>完整的代码可以参考<a href="https://github.com/cowarder/yolov3_voc/blob/master/k-means.py" target="_blank" rel="noopener">这里</a></p><p><strong>参考资料</strong></p><p>1.<a href="https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html" target="_blank" rel="noopener">Anchor Boxes for Object Detection</a><br>2.<a href="https://towardsdatascience.com/neural-networks-intuitions-5-anchors-and-object-detection-fc9b12120830" target="_blank" rel="noopener">Neural Networks Intuitions: 5. Anchors and Object Detection</a><br>3.<a href="https://zhuanlan.zhihu.com/p/49995236" target="_blank" rel="noopener">史上最详细的Yolov3边框预测分析</a><br>4.<a href="https://zhuanlan.zhihu.com/p/73024408" target="_blank" rel="noopener">Why anchor?</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>object detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>翻译：A Recipe for Training Neural Networks</title>
    <link href="undefined2019/11/04/dl-recipe/"/>
    <url>2019/11/04/dl-recipe/</url>
    
    <content type="html"><![CDATA[<p>原文链接：<a href="http://karpathy.github.io/2019/04/25/recipe/" target="_blank" rel="noopener">A Recipe for Training Neural Networks</a></p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>&emsp; 几周之前我发了一条推特<a href="https://twitter.com/karpathy/status/1013244313327681536?lang=en" target="_blank" rel="noopener">“最常见的神经网络错误”</a>，其中列出了几条在训练神经网络过程中常见的陷阱。这条推特比我预想的得到了更多的关注。很明显，很多人都亲身经历过”我们的网络能够work”与”我们的网络实现了state of the art的效果”之间的巨大性能差异。</p><p>&emsp; 所以我觉得如果将这条推特通过博客的形式来延伸一下应该很有趣。但是我并不打算列举出那些常见的错误，我想更加深层次的探讨一下如何去避免这些错误（或者迅速修正他们）。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="1-熟悉你的数据"><a href="#1-熟悉你的数据" class="headerlink" title="1.熟悉你的数据"></a>1.熟悉你的数据</h4><p>&emsp; 构建神经网络的第一步不是去写代码，而是充分的观察你的数据，这一步至关重要。我通常喜欢花费大量时间（小时为单位）来浏览数据样本，从而了解他们的分布规律并寻找数据中可能存在的模式。幸运的是，人类的大脑是非常擅长做这类工作的。通过浏览样本，可能这次我发现数据中包含重复样本，下一次我就能发现损坏的图像/标签。同时我还会寻找数据中的不平衡和偏差问题。通常，我会按照自己的方法将数据分类，这暗示了我们最终要探索的各种数据中的结构。例如：</p><p>&emsp; 仅仅采用局部特征就已经足够或者还需要全局上下文信息？<br>&emsp; 数据中会有多少种变化并且各自呈现出什么形式？<br>&emsp; 哪些变化是虚假的并通过预处理可以过滤掉？<br>&emsp; 空间位置是否重要或者我们希望将其平均池化？<br>&emsp; 数据中的细节信息有多重要？<br>&emsp; 我们能够将图像下采样到什么程度？<br>&emsp; 噪声标签数据有多少？</p><p>&emsp; 除此之外，由于神经网络是数据集的压缩版本，你可以通过可视化网络预测结果来判断其来源，也就是说，如果网络预测结果与数据中观察到的现象不一致，那么就说明可能存在问题。</p><p>&emsp; 一旦通过对数据的观察有了一定的感觉，就可以根据你的想法通过写一些简单的代码来搜索/过滤/排序数据，并可以可视化他们的分布以及沿任意轴向的离群值。离群值几乎总是能揭露在数据质量或者数据预处理中存在的问题</p><h4 id="2-设立一个端到端的训练-评估框架-构建一个基础的baseline"><a href="#2-设立一个端到端的训练-评估框架-构建一个基础的baseline" class="headerlink" title="2.设立一个端到端的训练/评估框架+构建一个基础的baseline"></a>2.设立一个端到端的训练/评估框架+构建一个基础的baseline</h4><p>&emsp; 既然我们充分了解了我们的数据，那是不是就意味着可以开始利用ASPP、FPN、ResNet等网络来进行训练了呢？答案是“NO“。接下来一步是建立一个完整的训练/评估框架，并通过一系列的实验来确保框架的正确性。在这个阶段，最好是选择一些比较简单的模型，例如线性回归，或者是一个非常小的卷积网络。我们将要训练这个模型、可视化损失值、利用模型作出预测，并且利用明确的假设来进行一系列的消融实验。</p><p><strong>Tips &amp; tricks:</strong></p><ul><li>​    <strong><em>固定随机数种子</em></strong>   一定要采取固定随机数种子的方法，这能够保证两次运行代码能够获得同样的结果，对于实验的复现十分重要</li></ul><ul><li><strong><em>简化</em></strong>   确保不要在这个阶段采取一些不必要的fancy trick。比如关闭数据增强。数据增强是一种正则化策略，但是在这个阶段引入可能会带来一些愚蠢的bug</li></ul><ul><li><strong><em>在评估过程中添加有效数据</em></strong>    当绘制验证阶段的损失值的时候，需要对整个验证数据集进行评估，而不是仅仅针对一个batch数据进行评估。我们追求的是准确性，为了确保我们的方法是明智的，有时就需要放弃部分时间效率</li></ul><ul><li><strong><em>确保loss的初始值是正确的</em></strong>   保证损失变量拥有一个正确的初始值。比如，如果正确的初始化了最后一层，应该在初始化时为softmax设置 -log(1/n_classes)。可以为L2回归，Huber损失等导出同样的默认值</li></ul><ul><li><strong><em>初始化</em></strong>   正确地初始化网络的最后一层。例如当你的回归目标是一些均值为50的值，那就应该将最后的偏差设置为50。如果数据不平衡，例如pos:neg的值为1:10，那就应该在logit上面设置偏差，使得网络预测初始化为0.1。正确设置这些参数将加快收敛速度并且避免了在网络训练的初期网络仅仅学习偏差的问题（hockey stick）</li></ul><ul><li><strong><em>人为基线</em></strong>   监控评价指标而不是损失值，这些指标是人为可解释的并且可检查的(例如准确率)，尽可能评估自己设定的评价指标并在模型之间比较。</li></ul><ul><li><strong><em>输入无关的baseline</em></strong>   训练一个输入无关的baseline（最简单的方法是将所有的输入值设置为0），这样要比实际插入数据而不将数据清零时候的效果要差。本质上是说：你的模型完全学会了从输入数据中提取信息了吗？</li></ul><ul><li><strong><em>在小的batch上面过拟合</em></strong>   在只有少量数据的batch上面过拟合。为了这样做，我们需要增加我们模型的容量（例如添加更多的层数）并确保我们可以获取最低loss值。我也喜欢可视化预测值和真实值，确保最终它们之间能够完美的拟合，从而得到最低的loss值。如果不能够在小batch上面实现过拟合，就无法进行下一阶段</li></ul><ul><li><strong><em>在数据传入网络之前可视化</em></strong>   可视化数据的最佳位置应当是在 output = model(input) 代码段之前。也就是说，如果你想准确可视化网络的输入内容，将原始数据张量和标签编码为可视化数据，这是唯一的可靠来源。我已经记不清这种方法曾多少次节省了我的时间并且揭露了在预处理阶段和数据增强阶段存在的问题</li></ul><ul><li><strong><em>可视化预测动态信息</em></strong>   我喜欢在训练过程中利用固定的测试数据来可视化模型的预测结果。“动态信息”指的是这些预测结果在训练过程中如何变化，这可以在训练过程中给你更好的intuition。很多时候，如果网络以某种方式一直在摆动，这就意味着网络很难去适应你的数据，这其实揭露的是模型的不稳定性。有些时候，过高或者过低的学习率也会造成这种抖动现象</li></ul><ul><li><strong><em>利用backprop传播来绘制依赖关系图</em></strong>   深度学习的代码中经常包含复杂的、向量化的和广播操作。我曾几次遇到的常见bug是，人们会弄错view和permute/transpose之间操作的区别，这样就在无意中搞混了批处理数据的维度。最让人难受的是，即便你这样搞错了，但是你的模型还是能够表现的很好，因为模型自己会学习去忽略这些问题。解决这个问题的方法之一是将loss设置为很小的值，例如将loss设置为样本i输出的总和，然后运行backprop过程达到输入数据，确保只在第i个样本上得到非0的梯度。同样的策略可以应用在模型方面，比如保证t时刻的模型仅仅依赖于1到t-1时刻的模型。更一般的情况下，梯度会提供给你各变量与神经网络依赖关系的信息，这在调试过程中是非常有用的。</li></ul><ul><li><strong><em>泛化特定的代码</em></strong>   这意味着去编写一些更加泛化的代码，但是我经常看见一些人在一开始就尝试写一些“泛化”的代码，这无异于“没有金刚钻，还揽瓷器活”。我喜欢针对我现在在做的事情编写一些更加特殊化的代码，首先应该是使得代码能够跑起来，下一步才是提高它的泛化性能。一般这种情况适应于向量化代码，我总是先会写出一个完整的循环版本，然后针对每个循环逐次将代码向量化。</li></ul><h4 id="3-过拟合"><a href="#3-过拟合" class="headerlink" title="3.过拟合"></a>3.过拟合</h4><p>&emsp; 到现在，我们已经充分了解了我们的数据，并且建立了一个端到端的训练评估系统。给定任意模型，我们能够计算我们相信的度量指标。同时我们还拥有一个输入无关的baseline，几个基础的baseline。接下来就可以迭代一个好的模型了</p><p>&emsp; 我采用的寻找好的模型的方法分为两个阶段：第一个阶段是选取一个足够大的模型确保其能够针对数据做到过拟合，第二个阶段是对模型正则化。这样做的原因是：如果我们的模型不能够实现一个很低的错误率，那就可能预示着其中存在一些问题或者错误的设置。</p><p><strong>Tips &amp; tricks:</strong></p><ul><li><strong><em>选择一个模型</em></strong>   为了达到一个较低的训练损失，应该为给定数据选择一个正确的模型结构。我的第一点建议是：<strong>别逞能</strong>。很多人上来就跟疯了一样像堆乐高似得往网络中加入各种模块，在早期一定要克制这种行为，我建议大家首先查找最相关的文献，然后将文献中最简单的结构复制粘贴到网络中，以实现良好的性能。例如在做图片分类任务的时候，直接复制粘提ResNet-50来运行就可以了</li></ul><ul><li><strong><em>Adam是安全的优化器</em></strong>   在构建baseline的早期阶段，我喜欢采用Adam优化，并且将学习率设置为3e-4。就我的经验来说，Adam对于超参数具有更高的宽容度，包括设置的不好的学习率。对于卷积网络，一个精心调试的SGD达到的性能几乎总是优于Adam，但是最优学习率所在的区域相对更加狭窄（难以调节）并且针对特定的问题会有所变化（注意，如果你用的是RNN结构或者是类似的序列模型的话，更加应该采用RNN）</li></ul><ul><li><strong><em>一次只复杂化一个方面</em></strong>   如果你有多个改进策略要应用到模型当中，我建议你一个一个的插入其中，并且保证每次都能使得模型获得预期的效果提升。一个有效的策略是，首先在较小的图片中实验，然后再应用到较大的图片中。</li></ul><ul><li><strong><em>不要相信学习率衰减默认值</em></strong>   如果你要在现有的任务上复用来自其他任务的代码，一定要小心学习率衰减的设置。不但需要针对不同的问题设置不同的学习率衰减调度策略，调度策略还需要根据当前epoch来调整，因为随着数据量的不同，调度策略变化的范围就会很大。例如，ImageNet每30个epoch降低10倍，如果你不是在训练ImageNet，你一般不会这样设置。如果不小心调整代码，可能学习率就会过早的趋于0，导致模型无法收敛。在我的日常工作中，我一般会将学习率设置为一个常数，不采用学习率衰减，直到最后才会对其进行调整。</li></ul><h4 id="4-正则化"><a href="#4-正则化" class="headerlink" title="4.正则化"></a>4.正则化</h4><p>&emsp; 理想情况下，到目前为止，我们已经有了一个至少在训练数据集上面表现良好的模型。接下来就应该是正则化操作了。通过牺牲一些训练集上面的准确率来获得验证集上准确率的提高。</p><p><strong>Tips &amp; tricks:</strong></p><ul><li><strong><em>获取更多的数据</em></strong>   首先，当前最好并且最流行的正则化网络模型方法就是扩充训练数据。耗费大量的工程周期来从小数据集中压榨模型效果是一个常见的错误，而你原本可以通过收集更多的数据来达到同样的效果。据我所知，增加训练数据几乎是无限提高神经网络性能的唯一保证方法（另外一个是ensemble）</li></ul><ul><li><strong><em>数据增强</em></strong>   尝试一些更加随意的数据增强手段</li></ul><ul><li><strong><em>预训练</em></strong>   如果你有足够多的数据，预训练一般不会使你的模型效果更差</li></ul><ul><li><strong><em>坚持使用监督学习</em></strong>   不要对无监督学习保太大期望。据我所知，无监督学习在现代计算机视觉任务上面并没有取得好的效果（即便是Bert在NLP任务上面取得了好的效果，但是可能只是由于文本的重复性特性和更高的信噪比）</li></ul><ul><li><strong><em>更小的输入维度</em></strong>   删除可能包含虚假信号的特征。如果数据集很小，则添加任何虚假信号都有可能过拟合。同样的，如果底层细节不重要，那么可以尝试输入较小的图像。</li></ul><ul><li><strong><em>更小的模型体积</em></strong>   很多情况下，你可以利用先验知识的限制来降低模型的体积。例如，过去常常在backbone的最上层采用一个全连接网络，但是如今却被简单的平均池化所代替，从而消除了大量的参数。</li></ul><ul><li><strong><em>更小的batch size</em></strong>   由于batch normalization中的归一化，更小的batch size一定程度上对应更强的正则化。因为batch上面的经验均值/标准差更近似于全局的均值/标准差，小的batch会带来更大的抖动</li></ul><ul><li><strong><em>dropout</em></strong>   对卷积采用2维dropout，但是在采用dropout的时候要注意，因为在有batch norm的时候，<a href="https://arxiv.org/abs/1801.05134" target="_blank" rel="noopener">dropout表现的不好</a> </li></ul><ul><li><strong><em>权重衰减</em></strong> 增加权重衰减惩罚</li></ul><ul><li><strong><em>early stopping</em></strong>   通过验证集损失的变化来停止训练，避免过拟合</li></ul><ul><li><strong><em>尝试采用更大的模型</em></strong>   我在最后才提这一点，因为我在实验中确实会发现更大的模型会导致过拟合，但是由于有了early stopping策略，大的模型往往会比小的模型效果好，所以我将其放在early stopping后面</li></ul><p>&emsp; 最后，为了证明你训练的是一个合理的分类器，我喜欢可视化网络第一层的权重，确保在这一层你能够获得一些有意义的图像边缘信息。如果在第一层就显得杂乱无章的话，可能就意味着出现了某些问题。同样，内部激活函数输出内容有时也会展现出来一些奇怪的现象，对应着预示了一些问题。</p><h4 id="5-调整"><a href="#5-调整" class="headerlink" title="5.调整"></a>5.调整</h4><p>&emsp; 现在，你应该为了降低验证集上的损失值，而陷入了探索广阔的模型空间的循环之中。在这一步有几点建议：</p><ul><li><strong><em>随机的网络搜索</em></strong>   当需要调整大量超参数的时候，采用网络搜索能够保证所有设置的覆盖范围，但是一定要记住最好是采用<a href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="noopener">随机网格搜索</a>。直观上来讲，神经网络针对一部分超参数更加敏感。如果参数a更加重要而改变参数b的值没有影响，那么你宁愿进行多次采样而不是在固定点上采样</li></ul><ul><li><strong><em>超参数优化</em></strong>   有很多精美的贝叶斯超参数优化工具，我的一些朋友也反映这些工具很好用</li></ul><h4 id="6-进一步提高模型效果"><a href="#6-进一步提高模型效果" class="headerlink" title="6.进一步提高模型效果"></a>6.进一步提高模型效果</h4><p>&emsp; 即便你已经找到了最好的模型结构，并且设置好了最佳的超参数，你仍可以采用一些其他的trick来提高模型的效果</p><ul><li><strong><em>ensemble</em></strong>   模型融合能够保证在任何任务上面大约能够获得2%的提升。如果你不能承担在测试阶段进行模型融合的计算量，可以尝试<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">知识蒸馏</a></li><li><strong><em>让模型持续训练</em></strong>   当验证集上面的损失值不再降低的时候，人们往往倾向于停止训练。根据我的经验，网络可以保持长期训练。有一次我在休冬假的时候，忘记关掉网络，它就一直训练，当我休完假回来之后，发现它达到了state of the art的效果</li></ul><h3 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h3><p>&emsp; 当你读到这里的时候，你已经具备了成功的所有因素：你对技术、数据集、问题有了很深的理解，你已经建立起了完整的训练/评估结构，并对其准确性抱有很高的信心。并且你已经探索了更加复杂的模型，并通过上述步骤获得了性能上面的提升。现在该去读paper，做实验，创造你自己的SOTA模型了。祝好！</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>梯度累加(Gradient Accumulation)</title>
    <link href="undefined2019/10/29/Gradient-Accumulation/"/>
    <url>2019/10/29/Gradient-Accumulation/</url>
    
    <content type="html"><![CDATA[<p>&emsp;  我们在训练神经网络的时候，超参数batch size的大小会对最终的模型效果产生很大的影响。一定条件下，batch size设置的越大，模型就会越稳定。batch size的值通常设置在 8-32 之间，但是当我们做一些计算量需求大的任务(例如语义分割、GAN等)或者输入图片尺寸太大的时候，我们的batch size往往只能设置为2或者4，否则就会出现 “CUDA OUT OF MEMORY” 的不可抗力报错。  </p><p>&emsp;  贫穷是促进人类进步的阶梯，如何在有限的计算资源的条件下，训练时采用更大的batch size呢？这就是<strong>梯度累加(Gradient Accumulation)</strong>技术了。  </p><p>&emsp; 我们以<a href="https://pytorch.org/" target="_blank" rel="noopener">Pytorch</a>为例，一个神经网络的训练过程通常如下：</p><pre><code class="python">    for i, (inputs, labels) in enumerate(trainloader):        optimizer.zero_grad()                   # 梯度清零        outputs = net(inputs)                   # 正向传播        loss = criterion(outputs, labels)       # 计算损失        loss.backward()                         # 反向传播，计算梯度        optimizer.step()                        # 更新参数        if (i+1) % evaluation_steps == 0:            evaluate_model()</code></pre><p>&emsp; 从代码中可以很清楚地看到神经网络是如何做到训练的：<br>                1.将前一个batch计算之后的网络梯度清零<br>                2.正向传播，将数据传入网络，得到预测结果<br>                3.根据预测结果与label，计算损失值<br>                4.利用损失进行反向传播，计算参数梯度<br>                5.利用计算的参数梯度更新网络参数</p><p>&emsp; 下面来看梯度累加是如何做的：</p><pre><code class="python">    for i, (inputs, labels) in enumerate(trainloader):        outputs = net(inputs)                   # 正向传播        loss = criterion(outputs, labels)       # 计算损失函数        loss = loss / accumulation_steps        # 损失标准化        loss.backward()                         # 反向传播，计算梯度        if (i+1) % accumulation_steps == 0:            optimizer.step()                    # 更新参数            optimizer.zero_grad()               # 梯度清零            if (i+1) % evaluation_steps == 0:                evaluate_model()</code></pre><p>&emsp;<br>                1.正向传播，将数据传入网络，得到预测结果<br>                2.根据预测结果与label，计算损失值<br>                3.利用损失进行反向传播，计算参数梯度<br>                4.<strong>重复1-3，不清空梯度，而是将梯度累加</strong><br>                5.梯度累加达到固定次数之后，更新参数，然后将梯度清零  </p><p>&emsp; <strong>总结来讲，梯度累加就是每计算一个batch的梯度，不进行清零，而是做梯度的累加，当累加到一定的次数之后，再更新网络参数，然后将梯度清零。</strong></p><p>&emsp; 通过这种参数延迟更新的手段，可以实现与采用大batch size相近的效果。在平时的实验过程中，我一般会采用梯度累加技术，大多数情况下，采用梯度累加训练的模型效果，要比采用小batch size训练的模型效果要好很多。</p><p><strong>参考资料</strong></p><p>1.<a href="https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3" target="_blank" rel="noopener">Loss Normalization</a><br>2.<a href="https://discuss.pytorch.org/t/model-zero-grad-or-optimizer-zero-grad/28426" target="_blank" rel="noopener">Model.zero_grad() or optimizer.zero_grad()?</a><br>3.<a href="https://www.kaggle.com/c/understanding_cloud_organization/discussion/105614#latest-643946" target="_blank" rel="noopener">A trick to use bigger batches for training: gradient accumulation</a><br>4.<a href="https://www.zhihu.com/question/303070254/answer/573037166" target="_blank" rel="noopener">PyTorch中在反向传播前为什么要手动将梯度清零？</a><br>5.<a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255" target="_blank" rel="noopener">Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>