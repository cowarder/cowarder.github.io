<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>docker简介及随手查</title>
    <link href="undefined2021/02/22/docker/"/>
    <url>2021/02/22/docker/</url>
    
    <content type="html"><![CDATA[<h3 id="环境配置问题"><a href="#环境配置问题" class="headerlink" title="环境配置问题"></a>环境配置问题</h3><p>程序员经常可以听到的一句话是：“代码明明在我电脑上面能跑的啊，怎么到你这就不行了？肯定是你电脑问题（不是）”。听上去是一个调侃，其实确实是经常遇见的问题，而这个问题出现的本质原因是由环境配置引起的。代码运行需要依赖于环境，比如操作系统的设置，各种库和组件的配置，可能还涉及到很多的环境变量，而这些东西都是定制化的，当我们换一台机器的时候，就需要重新再配置一次，可以预见，这个过程需要做许多重复性的工作，耗费大量精力，那么自然就可以想到，我们在拷贝一份代码或者运行一个软件的时候，能否同时将原市的环境也囊括进来？</p><h3 id="虚拟机"><a href="#虚拟机" class="headerlink" title="虚拟机"></a>虚拟机</h3><p>虚拟机是带着环境安装的一种解决方案。虚拟机是一种在操作系统里面运行的操作系统，对于底层系统来讲，虚拟机是一个普通文件，但是虚拟机会占用更多的资源，独占一部分内存和硬盘空间，虚拟机运行的时候，其他程序无法使用这些资源，即便是虚拟机里面的程序需要占用的内存很少，但是虚拟机仍然需要很大一部分资源才能够运行。而且虚拟机是操作系统级别的，启动起来非常慢。</p><h3 id="Linux容器"><a href="#Linux容器" class="headerlink" title="Linux容器"></a>Linux容器</h3><p>由于虚拟机存在的诸多问题，Linux发展了另外一种虚拟化技术：linux容器。linux容器不是一个完整的操作系统，可以将其视为是一种对于进程的隔离。对容器里面的进程而言，各种资源都是虚拟的，实现了与底层系统的隔离。</p><h4 id="启动迅速"><a href="#启动迅速" class="headerlink" title="启动迅速"></a>启动迅速</h4><p>容器里面的应用，就是底层系统的一个进程，不是虚拟机内部的进程，启动容器相当于启动一个进程，而不是启动一个操作系统，相较于虚拟机，它的启动更快。</p><h4 id="占用资源少"><a href="#占用资源少" class="headerlink" title="占用资源少"></a>占用资源少</h4><p>容器只会占用需要的资源，不会占用那些没有用到的资源，虚拟机是操作系统级别的，需要占用所有资源，多个容器之间可以共享资源，但是虚拟机都是独享的。</p><h4 id="体积小"><a href="#体积小" class="headerlink" title="体积小"></a>体积小</h4><p>容器只需要包含用到的组件，而虚拟机是整个操作系统的打包，所以容器要比虚拟机文件小的多</p><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p>docker是linux容器的一种封装，给用户提供简单易用的容器使用接口。docker将应用程序与该程序的依赖，打包在一个文件里面，通过运行这个文件，就会生成一个虚拟容器，程序在这个容器里面运行，就像是在真实的物理及上面运行一样。docker的接口相当简单，用户可以非常方便的创建和使用容器，把自己的应用放入容器，容器还可以进行版本管理、复制、分享、更新，就像是普通的代码一样</p><h3 id="Docker常见命令"><a href="#Docker常见命令" class="headerlink" title="Docker常见命令"></a>Docker常见命令</h3><p>docker把应用程序及其依赖，打包在image文件里。只有通过这个文件，才能够生成Docker容器。image文件可以看作是容器的模板。Docker根据image文件生成容器的实例。同一个image，可以生成多个同时运行的容器实例。  </p><p>列出所有image文件</p><blockquote><p>docker image ls</p></blockquote><p>删除image文件</p><blockquote><p>docker image rm [imagename]</p></blockquote><p>image文件是通用的，一台机器的image文件拷贝到另外一台机器后同样可以使用。为了共享方便，image文件制作完成后，可以上传到网上的仓库：<a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a>。</p><p>将image仓库抓取到本地</p><blockquote><p>docker image pull library/hello-world</p></blockquote><p>上述命令中library/hello-world代表的是image文件在仓库中的位置，其中library是image文件所在的组，hello-world代表的是image文件的名字，Docker官方提供的image文件都在library组里面  </p><p>运行image文件</p><blockquote><p>docker container run hello-world</p></blockquote><p>dock er container run命令会根据image文件，生成一个正在运行的容器实例，docker container run具有自动抓取的功能，如果在本地没有发现指定的image文件，就会从仓库自动抓取</p><p>终止container运行</p><blockquote><p>docker container kill [containerid]</p></blockquote><p>image文件生成的容器实例，本身也是一个文件，称为容器文件。也就是说，一旦容器生成，就会同时存在image文件和容器文件，而且关闭容器不会删除容器文件，只是容器停止运行而已。</p><p>列出正在运行的容器</p><blockquote><p>docker container ls</p></blockquote><p>列出所有的容器</p><blockquote><p>docker container ls –all</p></blockquote><p>删除容器</p><blockquote><p>docker container rm [containerid]</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3D目标检测算法调研</title>
    <link href="undefined2020/12/07/3d-detection/"/>
    <url>2020/12/07/3d-detection/</url>
    
    <content type="html"><![CDATA[<p>刚刚来到百度实习，方向是3D目标检测，前期工作是做算法调研，撰写本篇博客略作记录  </p><p>目前，2D目标检测算法已经发展的比较成熟，但是在无人驾驶、机器人、增强现实的应用场景下，普通的2D检测并不能提供感知环境所需要的全部信息，2D检测仅能提供目标物体在二维图片中的位置和对应类别的置信度，但是在真实的三维世界中，物体都是有三维形状的，大部分应用都需要有目标物体的长宽高还有偏转角度等信息。目前的3D目标检测正处于高速发展时期，目前主要是综合利用单目相机、双目相机、多线激光雷达来进行3D目标检测，从成本上来讲，激光雷达&gt;双目相机&gt;单目相机，从目前的准确率上来讲，激光雷达&gt;双目相机&gt;单目相机。但是随着激光雷达的不断产业化发展，成本在不断降低，目前也出现了一些使用单目相机加线数较少的激光雷达进行综合使用的技术方案  </p><p>因为在百度实习所做的方向是基于激光点云数据的3D目标检测，因此主要对基于点云的方法进行探讨。从点云数据表示的角度来讲，我们可以将现有的主流检测方法分为两类：一种是voxel-based的方法(将不规则的点云数据转换为规则数据)，另外一种就是point-based的方法(直接在原始的点云数据上面处理)。前者的代表算法有voxelnet、second、pointpillars等，后者的代表性算法有pointnet、pointnet++、pv-rcnn等算法</p><h3 id="voxel-based"><a href="#voxel-based" class="headerlink" title="voxel-based"></a>voxel-based</h3><h4 id="VoxelNet"><a href="#VoxelNet" class="headerlink" title="VoxelNet"></a>VoxelNet</h4><p>voxelnet是基于voxel的方法的开山之作，将三维点云划分为一定数量的voxel，经过点的随机采样和归一化，对每一个非空voxel使用若干个VFE(Voxel Feature Encoding)层进行局部特征提取，得到voxel-wise feature，然后经过3D Convolutional Middle Layers进一步抽象特征(增大感受野并学习几何空间表示)，最后使用RPN(Region Proposal Network)对物体进行分类检测和位置回归。VoxelNet整个pipeline如下图所示</p><center><img src="/2020/12/07/3d-detection/voxelnet.png" srcset="undefined" , height="200"></center><p>其中，RPN网络结构如图</p><center><img src="/2020/12/07/3d-detection/rpn.png" srcset="undefined" , height="150"></center><p>voxelnet做出了非常大的突破，对如何规格化利用点云信息作出了贡献，但是voxelnet中采用了3D卷积，导致其运算量很大，占用内存过多，因此，second模型被提出</p><h4 id="Second"><a href="#Second" class="headerlink" title="Second"></a>Second</h4><p>second网络在结构上面并没有太大的创新，主要是采用了稀疏卷积来代替传统的3D卷积，大大提高了网络的计算速度，减少了内存的占用</p><center><img src="/2020/12/07/3d-detection/second.png" srcset="undefined" , height="150"></center><h4 id="稀疏卷积"><a href="#稀疏卷积" class="headerlink" title="稀疏卷积"></a>稀疏卷积</h4><p>传统的卷积操作，随着网络层的不断加深，会破坏原有数据的稀疏性，原本稀疏的数据会随着网络层不断加深逐渐变得平滑，如下图所示</p><center><img src="/2020/12/07/3d-detection/spconv1.png" srcset="undefined" , height="80"></center><p>而稀疏卷积在运算过程中只关注中心点区域，也就是只关注激活区域本身，并且不会造成数据稀疏性的损失，同时大大减少了计算中的运算量，稀疏卷积效果如图所示</p><center><img src="/2020/12/07/3d-detection/spconv2.png" srcset="undefined" , height="80"></center><h4 id="PointPillars"><a href="#PointPillars" class="headerlink" title="PointPillars"></a>PointPillars</h4><p>pointpillars也是基于voxel的一篇文章，也是目前基于点云的3D目标检测算法中最快的一个，该算法提出一种新的特征编码网络，在体素的垂直列上面不做切割，在鸟瞰图下，将点云数据编码为稀疏的伪图像，利用2D卷积的目标检测算法进行特征提取和检测，从而移除了3D卷积的计算负担</p><center><img src="/2020/12/07/3d-detection/pillars.png" srcset="undefined" , height="200"></center><h3 id="point-based"><a href="#point-based" class="headerlink" title="point-based"></a>point-based</h3><h4 id="PointNet"><a href="#PointNet" class="headerlink" title="PointNet"></a>PointNet</h4><p>PointNet是直接在点云数据上面提取特征的第一篇文章，是直接处理点云数据方法的开端，整体思想非常简单直观：输入为所有无序的点云坐标，经过一系列的mlp(multi-layer perceptron)对所有点进行特征编码，每个点最终用一个特征向量来表示，然后综合所有点生成一个全局特征向量，再将全局特征向量与每个点的特征向量进行融合。整体思想非常简单，甚至比CNN的结构都要简单，但却是一篇开创性的工作。PointNet整体结构如下图所示</p><center><img src="/2020/12/07/3d-detection/pointnet.png" srcset="undefined" , height="200"></center><h4 id="PointNet-1"><a href="#PointNet-1" class="headerlink" title="PointNet++"></a>PointNet++</h4><p>从PointNet的描述中我们可以看到，网络利用了每个点的特征以及全局特征，但是并没有像CNN那样利用局部特征，因此PointNet++在PointNet的基础上面，提出了SA(Set Abstraction)操作，充分利用了数据中的局部信息，set abstraction操作分为三层  </p><blockquote><p>1.Sampling layer: 对输入点进行采样，选取出若干个中心点，采用FPS(Farthest Point Sampling)方法</p><p>2.Grouping layer: 利用上一步获得的中心点将点集划分为若干个区域</p><p>3.Pointnet layer: 对每个区域进行编码，转换为特征向量</p></blockquote><center><img src="/2020/12/07/3d-detection/sa.png" srcset="undefined" , height="200"></center><h4 id="PV-RCNN"><a href="#PV-RCNN" class="headerlink" title="PV-RCNN"></a>PV-RCNN</h4><p>set abstraction在原始点云数据上面做，保留了准确的位置信息，通过自定义的球半径，感受野更加灵活。稀疏卷积在voxel方法上面使用，速度往往更快，产生的proposal往往质量更高。PV-RCNN综合利用了这两种特征提取方法，在精度上面取得了很大的进步  </p><center><img src="/2020/12/07/3d-detection/pvrcnn.png" srcset="undefined" , height="250"></center><p>上图为pvrcnn模型整体结构，本文的主要贡献可以分为三点</p><blockquote><p>1.提出了Voxel Set Abstraction操作，将Sparse Convolution主干网络中多个scale的sparse voxel及其特征投影回原始3D空间，然后将少量的keypoint (从点云中sample而来) 作为球中心，在每个scale上去聚合周围的voxel-wise的特征。这个过程实际上结合了point-based和voxel-based两种点云特征提取的结构，同时将整个场景的multi-scale的信息聚合到了少量的关键点特征中，以便下一步的RoI-pooling</p><p>2.提出了Predicted Keypoint Weighting模块，通过从3D标注框中获取的免费点云分割标注，来更加凸显前景关键点的特征，削弱背景关键点的特征</p><p>3.设计了更强的点云3D RoI Pooling操作，也就是提出的RoI-grid Pooling: 与前面不同，在每个RoI里面均匀的sample一些grid point，然后将grid point当做球中心，去聚合周围的keypoint的特征。这样做的好处有两个: (1) 球半径灵活，甚至可以包括proposal框外围的关键点，从而获取更多有效特征。(2) 球互相覆盖，每个keypoint可以被多个grid point使用，从而得到更紧密的RoI特征表达。另一方面，这其实也是另一个point (keypoint)与voxel (grid point)特征交互的过程</p><center><img src="/2020/12/07/3d-detection/roi.png" srcset="undefined" , height="200"></center></blockquote><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>1.<a href="https://arxiv.org/abs/1711.06396" target="_blank" rel="noopener">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a></p><p>2.<a href="https://www.mdpi.com/1424-8220/18/10/3337" target="_blank" rel="noopener">SECOND: Sparsely Embedded Convolutional Detection</a></p><p>3.<a href="https://arxiv.org/abs/1711.10275" target="_blank" rel="noopener">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</a></p><p>4.<a href="https://arxiv.org/abs/1812.05784" target="_blank" rel="noopener">PointPillars: Fast Encoders for Object Detection from Point Clouds</a></p><p>5.<a href="https://arxiv.org/abs/1612.00593" target="_blank" rel="noopener">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</a></p><p>6.<a href="https://arxiv.org/abs/1706.02413" target="_blank" rel="noopener">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a></p><p>7.<a href="https://arxiv.org/abs/1912.13192" target="_blank" rel="noopener">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>实习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读《Temporal Shift Module for Efficient Video Understanding》</title>
    <link href="undefined2020/10/22/tsm/"/>
    <url>2020/10/22/tsm/</url>
    
    <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>视频理解领域，高准确率和低计算成本的权衡一直是一个十分重要的问题。传统的2D卷积计算量很小，但是只能捕捉图像中的空间信息，无法捕捉帧与帧之间的时序信息。基于3D卷积的方法虽然效果很好，但是计算成本太高，实际部署困难。这篇文章中，我们提出了一种泛化性强、效率高的模块(Temporal Shift Module)，能够同时实现高效和高精度，能够实现3D卷积精度的基础上，仍然保持2D卷积的计算成本。TSM在时间维度上面移动了部分通道，便于在相邻帧图像之间交换信息。TSM模块可以插入2D卷积模型中，在不增加任何计算成本的条件下实现时间建模。我们还将TSM应用在在线任务中，实现了实时低延迟的在线视频识别和视频对象检测。TSM准确而有效，在Something-Something数据集榜单上面排行第一，在Galaxy Note8手机上面，实现了35ms低延迟的在线视频识别。代码可以在<a href="https://github.com/mit-han-lab/temporal-shift-module" target="_blank" rel="noopener">这里</a>找到  </p><h4 id="TSM模块"><a href="#TSM模块" class="headerlink" title="TSM模块"></a>TSM模块</h4><p>TSM通过沿着时间维度移动特征图通道来执行有效的时间建模。相较于2D卷积，并没有增加计算量，但是TSM结构具有强大的时间建模能力。TSM能够有效支持在线和离线视频分类任务，双向TSM将过去的帧和未来的帧以及当前帧进行混合，适应于高吞吐量的离线视频识别。单向TSM将过去的帧与当前帧进行混合，适应于低延迟的在线视频识别 </p><center><img src="/2020/10/22/tsm/tsm.png" srcset="undefined"></center><p>具体来讲，视频任务的每一层feature map我们可以将其视为是一个$R^{NxCxTxHxW}$矩阵，其中N代表batch size，C代表的是feature的通道数，T代表的是时间维度，H和W代表的是空间维度。传统的2D卷积沿着维度T独立应用在每一帧图像上，因此没有利用时间信息(图a所示)。相比下，TSM沿着时间维度同时向前向后移动channel(图b所示)，通过移动channel，相邻帧图像的信息与当前帧进行了混合。这种思想的出发点是：卷积运算包括位移和乘法累加，我们将时间维度偏移+-1，并将乘积从时间维度折叠到通道维度。为了实现在线视频理解，将来的帧不能与当前帧进行融合，因此提出了单向TSM(图c所示)   </p><p>考虑一个简单的卷积操作，这里为了简化，采用1维卷积，kernel size设置为3。假设卷积的权重是$W=(w_1, w_2, w_3)$，输入是向量$X$，卷积操作可以表示为$Y=Conv(W, X)$，分解$Y_i = w_1X_{i-1}+w_2X_i+w_3X_{i+1}$，我们可以将卷积操作分解为两步：位移(shift)以及乘法累加(multiply-accumulate)，我们利用-1, 0, +1位移$X$，然后利用$w_1, w_2, w_3$做乘积，累加之后得到对应的$Y$。shift操作表示为:</p><p>$$X_{i}^{-1}=X_{i-1}, X_i^0=X_i, X_i^{+1}=X_{i+1}$$</p><p>乘法累加操作表示为</p><p>$$Y=w_1X^{-1}+w_2X^0+w_3X^{+1}$$</p><p>第一步shift操作可以在没有任何乘法的条件下实现，第二步的乘法累加计算量更大，TSM模块将乘法累加操作融合到了接下来的2D卷积中去，因此相较于原来的2D卷积操作，并没有造成任何额外的计算损失</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><pre><code class="python">class TemporalShift(nn.Module):    def __init__(self, net, n_segment=3, n_div=8, inplace=False):        super(TemporalShift, self).__init__()        self.net = net        self.n_segment = n_segment        self.fold_div = n_div        self.inplace = inplace        if inplace:            print(&#39;=&gt; Using in-place shift...&#39;)        print(&#39;=&gt; Using fold div: {}&#39;.format(self.fold_div))    def forward(self, x):        x = self.shift(x, self.n_segment, fold_div=self.fold_div, inplace=self.inplace)        return self.net(x)    @staticmethod    def shift(x, n_segment, fold_div=3, inplace=False):        nt, c, h, w = x.size()        n_batch = nt // n_segment        x = x.view(n_batch, n_segment, c, h, w)        fold = c // fold_div        if inplace:            # Due to some out of order error when performing parallel computing.             # May need to write a CUDA kernel.            raise NotImplementedError              # out = InplaceShift.apply(x, fold)        else:            out = torch.zeros_like(x)            out[:, :-1, :fold] = x[:, 1:, :fold]  # shift left            out[:, 1:, fold: 2 * fold] = x[:, :-1, fold: 2 * fold]  # shift right            out[:, :, 2 * fold:] = x[:, :, 2 * fold:]  # not shift        return out.view(nt, c, h, w)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>平安科技实习总结</title>
    <link href="undefined2020/10/05/pingan/"/>
    <url>2020/10/05/pingan/</url>
    
    <content type="html"><![CDATA[<p>今年7月份到9月份在平安科技(上海)实习了一段时间，岗位是人脸识别算法组算法工程师，主要负责采用深度学习进行视频流的唇语识别。平安科技不愧是保险公司，就是有钱，当时组里有四个实习生，我做唇语识别，另外三个人做GAN、声形同步、以及3D目标检测，平均每个人有一台8卡Tesla v100可以用，有充足计算资源自己折腾实验  </p><p>当时我负责的研究内容是读验证码唇语识别，给定的数据是公司的内部数据，每个样本是一个短视频，里面显示的是不同的人读取数字，实习期间针对这个任务做了一系列的改进 。整体大致流程是：输入视频数据-&gt;提取音频-&gt;根据音频截取视频中间说话人的部分-&gt;从截取视频中抽取固定帧图像-&gt;训练  </p><h4 id="1-数据分析"><a href="#1-数据分析" class="headerlink" title="1.数据分析"></a>1.数据分析</h4><p>前期拿到数据首先做了初步的分析，数据比较脏，存在诸多问题：</p><blockquote><p>(1) 噪音较多。背景音比较嘈杂</p><p>(2) 方言。导致同一验证码唇形变化不一致</p><p>(3) 重复读。一个验证码有的人会读多次</p><p>(4) 截断。并没有完整截取视频片段</p><p>(5) 唇动不明显。这种情形更多是个人习惯导致</p></blockquote><p>噪音问题会导致我们在截取视频前后片段时的不准确，从而导致采样图片帧出现较大误差，如何从视频中抽取富含信息的图像帧，直接决定了最终模型能够达到的识别精度  </p><h4 id="2-数据过滤"><a href="#2-数据过滤" class="headerlink" title="2.数据过滤"></a>2.数据过滤</h4><p>这一步主要是过滤了一些较短或者较长的视频，根据时间长短筛选视频，通过对筛选出的数据做分析，发现较短的视频基本都是没有读完，而较长的视频，部分是不相关数据，部分是采样时间过长的数据  </p><h4 id="3-视频截取"><a href="#3-视频截取" class="headerlink" title="3.视频截取"></a>3.视频截取</h4><p>由于视频中大多在开始以及结束阶段存在一个空白区域，在这一段时间内视频中人并没有说话，唇形没有发生变化，所以我们需要将视频的开始以及结束阶段都去掉，只截取中间说话的部分，这里我们采用的思路是根据音频来判断说话起止时间，设计了两种算法：</p><blockquote><p>(1) 首先计算音频wav的均值，在时间轴上面找出大于均值的起止时间，然后利用新的起止时间重新计算均值，找到新的起止时间，迭代次数人为设定</p><p>(2) 因为我们的验证码都是4个数字，对音频wav做k(k=4)均值聚类，这样可以找到第一个和第四个数字的读取时间</p></blockquote><p>实验中发现，第一种方法速度快，但是精度不高，更加容易受到噪音的影响，截取效果不是很理想，第二种方法虽然截取精度高，但是却非常慢，综合考虑下采用了3次迭代下的第一种算法，也尝试了根据相邻帧之间唇形是否发生变化来判断起止时间，但是时间消耗非常大，无法达到线上使用的要求  </p><p>为了消除噪音的影响，采用了<a href="https://github.com/deezer/spleeter" target="_blank" rel="noopener">spleeter</a>，spleeter能够将背景音乐与人声分离，我们将这个库应用在我们的项目中，消除了部分视频中的噪声，进一步提升了视频截取的精度  </p><h4 id="4-视频抽帧"><a href="#4-视频抽帧" class="headerlink" title="4.视频抽帧"></a>4.视频抽帧</h4><p>在探索如何抽取帧以及抽取多少帧这一阶段也做了较多工作，首先是抽帧数量，我们尝试了5-25张(每隔5张)，比5更少的话有效信息不足，如果更多的话我们在训练模型的时候成本会增加，但是由于视频截取的不完美，总会出现视频的前几帧或者后几帧冗余的现象，为了增强抽帧的鲁棒性，最终采取的策略是：</p><blockquote><p>每个视频抽取10-25帧图像，然后在开头以及结尾进行padding(copy-paste)，最终得到固定的24帧，这样的话就缓解了前后冗余帧的问题</p></blockquote><p>在抽帧策略上面，尝试了均匀采样以及随机采样，均匀采样效果要好一些，因此最终选择均匀采样  </p><h4 id="5-唇部提取"><a href="#5-唇部提取" class="headerlink" title="5.唇部提取"></a>5.唇部提取</h4><p>因为不同视频场景不尽相同，因此视频帧图像无法直接用于唇语识别，需要做进一步提取   </p><p>初始的思路是手动标注了大约1000张人脸图像，利用yolov3进行人脸(唇部)检测，标注的时候标注的是人脸下颌到鼻子之间的区域，并不包含全部人脸，且保证嘴巴在图片的正中心，训练后的模型切割效果良好  </p><p>最后的方案是利用dlib68人脸关键点检测算法，检测鼻尖、下颌、左右嘴夹关键点，分别按照比例左右上下进行扩充，检测的效果很好，并且速度也较快  </p><h4 id="6-模型选择"><a href="#6-模型选择" class="headerlink" title="6.模型选择"></a>6.模型选择</h4><h5 id="3D卷积模型"><a href="#3D卷积模型" class="headerlink" title="3D卷积模型"></a>3D卷积模型</h5><p>初期采用的是<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper.pdf" target="_blank" rel="noopener">《ECO: Efficient convolutional Network For online video understanding》</a>，这篇文章是3D卷积模型的代表作之一，也是我们首先尝试的模型，模型结构如下：</p><center><img src="/2020/10/05/pingan/eco.png" srcset="undefined"></center><blockquote><p>1.S1到SN是从视频中采样的N帧图像，对于每帧图像，采用权值共享的2D卷积子网络得到96个28*28的feature map，堆叠之后得到一个N*28*28*96的特征volume，到这一步采用的是BN-inception网络中的第一部分(inception-3c层前)</p><p>2.对于得到的特征volume，采用一个3D子网络进行处理，直接输出对应类别数量的一维向量，这一部分采用了3D-resnet18的部分层。以上的两个部分，就构成了ECO的网络</p></blockquote><h5 id="2D卷积模型"><a href="#2D卷积模型" class="headerlink" title="2D卷积模型"></a>2D卷积模型</h5><p>2D 卷积模型由于其轻量的结构和多元的 backbone 选择，使得其在云端计算和边缘部署上相对 3D 卷积模型具有巨大的优势，但 2D 卷积模型因为时序上的关系很难利用好，各种加如 lstm 等结构的网络还是被 3D 卷积实力碾压，通常情况下如果只考虑精度则不考虑 2D 卷积模型 。但是如果一个模型只使用 2D 卷积，且在分类精度上仍然能达到 SOTA，那么其带来的实用价值是巨大的，是碾压一切 3D 模型的存在。而2019年 ICCV 上的这篇文章<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.pdf" target="_blank" rel="noopener">《TSM: Temporal Shift Module for Efficient Video Understanding》</a>提出的 Temporal Shift Module 这个模块很好的利用了帧间的时序关系，实现了 3D 卷积的性能且保持了 2D 卷积的相对较小的计算量。最终采用的模型也正是2D卷积网络+TSM模块的结构  </p><center><img src="/2020/10/05/pingan/tsm.png" srcset="undefined"></center><p>如上图所示TSM模块通过沿时间维度移动特征图来执行有效的时间建模，图a是原始的tensor，图b是离线模式所适用的双向TSM将过去和未来的帧和当前帧混合，而图c是在线模式，实时识别，由于不能获取到未来的帧信息，所以仅用过去的帧和当前的帧混合在一起。我们最终采用的是图b的方式。可以看到，仅仅是对部分通道进行了移位，几乎没有额外的其他操作，也意味着没有额外的计算消耗，且没有多余的参数  </p><h4 id="7-数据增广"><a href="#7-数据增广" class="headerlink" title="7.数据增广"></a>7.数据增广</h4><p>对帧图像采样进一步调整，均匀将视频划分为几个片段，然后从每个片段中随机选择一幅图像，以此来增强模型的泛化能力，同时减少计算量。除此之外，我们还增加了随机丢帧的操作，具体做法是对样本图像数量大于一定值（如12）的样本进行随机丢帧，当然帧数小于2且不能为连续帧。另外还加入了图片随机移动，让一组嘴巴并不在一个位置，来增强模型的泛化能力   </p><p>对于数据增广，我们还采用了一些常规的增广方式：随机加入高斯噪声，随机对图像做双边滤波，随机调图像的明暗度、对比度、饱和度、色度，随机对图像加入小旋转(5°以内)，随机对图像做上下左右小平移，随机水平反转操作。这些数据增广操作都是在不损失模型的关键信息的原则上，去告诉模型它应该关注的信息是什么，以此来增强模型的泛化能力  </p><h4 id="8-模型训练"><a href="#8-模型训练" class="headerlink" title="8.模型训练"></a>8.模型训练</h4><p>因为原始的数据是4个数字，如果按照传统方法，这是一个万类分类问题，但是我们数据只有几万条，显然是不够的，因此将视频帧按照前后75%进行切割，做3个数字的分类，也就是1000类，这样模型更加容易收敛  </p><p>因为我们利用了额外的数据，训练初期出现模型迟迟无法收敛的问题，但是利用在外部数据上面训练的模型进行初始化之后，模型正常收敛，所以很多情况下如果出现模型无法收敛的问题，初始化也是一种良好的解决方案  </p><h4 id="9-外部数据"><a href="#9-外部数据" class="headerlink" title="9.外部数据"></a>9.外部数据</h4><p>采用了一万条中文词语数据，中文成语或者词语相比于数字，唇形变化明显，更加易于区分，通过对不同词语的准确率召回率进行分析，模型训练得到的效果更好，但是经过沟通后在业务上面行不通，因为考虑到很多用户并不认识字，但是数字大部分都是认识的，同理，英文字母验证码的方案也被pass掉了。最终我们采用的评估方案是正样本接受率以及负样本拒绝率，要求在副样本拒绝率90%以上的情况下尽可能提高正样本接受率，最终方案达到93%</p>]]></content>
    
    
    
    <tags>
      
      <tag>实习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习与深度学习指北</title>
    <link href="undefined2020/09/28/mldl/"/>
    <url>2020/09/28/mldl/</url>
    
    <content type="html"><![CDATA[<p>这篇文章是我之前写给同实验室的师弟师妹们看的，没有外传过，如果机器学习基础不牢固，就直接搞深度学习，我觉得不太可取，但是问了问他们又没有一个明确的学习计划，为了能让他们尽量少走弯路，尽快搞定深度学习&amp;机器学习的基础，写了这篇文章。这篇文章主要是记录我的学习过程，主观性可能比较强，仅供参考，学不好本人概不负责(QAQ)</p><h3 id="机器学习-amp-深度学习指北"><a href="#机器学习-amp-深度学习指北" class="headerlink" title="机器学习&amp;深度学习指北"></a>机器学习&amp;深度学习指北</h3><h4 id="前言"><a href="#前言" class="headerlink" title="前言:"></a>前言:</h4><p>本文将从作者学习经历出发，提炼出一条学习路线。网上的相关学习资料层出不穷，存在大量低质资源(各种教授、高级工程师出的 网课、书籍)，且存在很大程度上面的内容交叉，作者在学习的过程中也走过很多弯路，因 此本文的主要目的是让初学者对机器学习、深度学习迅速构建起一个知识脉络，避免走太多 的弯路，浪费宝贵时间。首先声明本文可能具有较大的主观性，因此内容仅做参考。</p><p>首先提出两点误区，1)上来就搞深度学习。深度学习只是机器学习的一小部分，应该 是对机器学习有了一定的基础之后，再去研究深度学习，没有机器学习知识储备就去搞深度 学习是不可取的，很容易成为一个调参工程师。2)重理论，轻实践。很多人只看书，把书 里面的公式推一推，就以为自己已经掌握了这个知识点，其实不然，计算机领域还是代码为 先，实践是检验真理的唯一标准，写代码掌握和推公式掌握是完全两种不同的理解。</p><h4 id="正文"><a href="#正文" class="headerlink" title="正文:"></a>正文:</h4><h5 id="1-机器学习"><a href="#1-机器学习" class="headerlink" title="1.机器学习:"></a>1.机器学习:</h5><p>网课首推的是 cs229，主讲人为吴恩达，吴恩达机器学习有两门课，一门是 08 年的 cs229，另外一门是 coursera 上面的机器学习，coursera 上面课程涵盖的内容甚至没有 cs229 的一半多，对于初学者太友好了，以至于学完了收获不大。cs229 在 2018 年进行了 重新的录制，08 版和 18 版在 bilibili 上面都有视频。这门课的讲义是课程的精华，仅仅是讲 义，价值就已经比很多机器学习课程高了。建议学习方法:看一节课，跟着他一起推导，视 频里出现的公式都搞懂，看完一节课之后去看这门课的讲义，进一步巩固。另外两个老师(林 轩田和李宏毅)的机器学习视频也都看过，但是首推的还是 cs229，虽然学习过程艰难，但 是收获会很大。(需要具备线性代数、微积分的理论基础)</p><p>看视频相对于看书，知识获取的效率是要低很多的，但是对于初学者很友好，因此首先 推荐的是视频，当看完 cs229 之后，如果是将里面的每一个点都搞懂了，那么恭喜，你已 经对机器学习领域有了一个基本的了解，接下来就是实践。推荐《机器学习实战》这本书， 这本书对于常见的机器学习算法(LR、SVM、KNN、树回归等)提供了代码演示，这本书 同时提供了源代码以及演示的数据，且在每一章都提供了结果可视化代码，更加直观，利用 这本书，你能够学到如何从数据中构建一个模型，如何训练、验证、展示结果。建议学习方 法:首先看懂知识点，根据思路尝试自己去实现，实现不了看代码，再尝试实现，重复这个 过程之后如果仍然无法实现，照着代码逐行写入，尽量不要直接复制粘贴，这样收获微乎其 微。</p><p>在学习《机器学习实战》的过程中可以进一步巩固理论知识，这里推荐的就是李航老师 的小蓝本《统计学习方法》，这本书一共有几百个公式，需要全部搞懂，是全部，每一个公 式你都需要推一遍，第一遍可能很困难，但是坚持下来，你的机器学习理论水平又会有比较 大的提升，推公式的意义不是为了跟别人谈及的时候显得自己很牛，而是帮助你更好地去理 解这个知识点。你能说出来算法的流程，这是外在的，而公式推导，是一种内在的东西。这 本书基本最少你得推个两遍。</p><p>到这里，你的理论基础就比较好了，基本代码也会写一些，但是还是不够，建议去 kaggle 上面去做泰坦尼克和房价预测的项目，一个回归一个分类，竞赛提供给你数据，利用你学到 的各种算法尝试去解决这两个问题，kaggle 上面最重要的就是 notebooks，读别人的代码， 你可以学到:如何做数据分析、数据清洗、数据预处理、eda、构建模型、训练测试 pipeline 的搭建、训练结果可视化、模型集成，以及各种提高模型效果的 trick，这些在实际项目中 都是十分重要的，还可以发现很多有用的工具包。当然，如果你要去做一些能够拿钱的正式比赛也是可以的。 好了，机器学习学到这里已经比较好了，大多数场景下都是够用了，接下来就是学深度学习。</p><h5 id="2-深度学习"><a href="#2-深度学习" class="headerlink" title="2.深度学习"></a>2.深度学习</h5><p>深度学习说到底其实没有什么太过神秘的东西，本质上就是卷积，其实就很简单。深度学习的视频就比较多了，其实水平也差不多，coursera 上面 deellearning.ai 的系列课程、 李飞飞的 cs231n，这些在 bilibili 上面都能找到，这两个我都看了，deeplearning.ai 很基础， 很友好，cs231n 更多是讲的计算机视觉里面的 deep learning，没涉及到 nlp 里面比如 rnn、 lstm 这些，后面我想巩固 rnn 这一块的是看了李宏毅深度学习课程里面的相关章节。总体 推荐还是 deeplearning.ai&gt;cs231n&gt;李宏毅。</p><p>深度学习是一门实验性的学科，因此接下来我建议你去学习一个框架，目前主流的 pytorch、tensorflow、paddlepaddle、keras，pytorch 和 tensorflow 分别是在学术界和工 业界用的最多的两个框架，paddlepaddle 是百度的，keras 内核基于 tensorflow，但是接口 简化，更简单。研究生首推 pytorch，生态很好，各种 bug 一般都能找到解决方案，版本差 距不大，简单易用，tensorflow 不同版本之间差距太大，而且有的概念比较抽象，对于新手 不友好，而且目前开源的最新论文代码基本都是基于 pytorch 的。打开 pytorch 官网 -&gt;tutorials，这里有很多入门教程，首推 DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ，学完这个你就对 pytorch 构建训练 pipeline 有了了解，然后你再在这个目录下面找 自己感兴趣的内容去跑一遍，关键点检测、目标检测、文本分类、强化学习。跑个一两个， 恭喜，你就算是入门了 pytorch，就是这么简单。</p><p>如果觉得还不够呢，就去复现一篇论文吧，比如可以复现经典的 GAN，WGAN，因为 GAN 思想很简单，复现所需要的代码很少，也比较有意思。这样能够让你的 pytorch 基础 进一步巩固。可以去 github 上面搜索 pytorch gan，参考别人的代码。</p><h5 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h5><p>接下来你已经具备了机器学习和深度学习的基础，就可以根据自己的研究方向开始看论文了，你会发现论文中提到的机器学习、深度学习概念大多数你都了解，即便不了解去搜一 下也能看懂。很多新的内容都需要你去从论文中学习。接下来可以考虑运用自己学会的东西 做更多的实践，可以复现一两篇跟自己研究方向相关的经典论文，还可以去参加一些大型的 比赛，比如 kaggle、天池等。在复现论文、做比赛的过程中，你会遇到各种问题，然后需 要去阅读更多的论文，再将论文里面的内容转化为代码进行实验，这就是一种理论-代码不 断迭代加强的学习路线，收获更多，成长更快。</p><h5 id="4-附录"><a href="#4-附录" class="headerlink" title="4.附录"></a>4.附录</h5><p>(1)《机器学习》(西瓜书)。西瓜书更多的是科普书籍，强调的是机器学习的一个广度，可以当作一个科普读物来看。课后题有的难度还是比较大的。 (2)《深度学习》(花书)。首先花书中文版翻译的不是很好，有很多概念类似于机翻，比较晦涩，不太适合阅读，如果想巩固深度学习的理论基础，还是看英文版对应的章节 就好，没有必要全看，遇见问题还是找对应的论文看比较好。</p><p>(3)《Pattern Recognition and Machine Learning》(PRML)。经典巨著之一的 PRML，如果有时间还是建议从头到尾读一遍英文原版的，我只看了前半部分，收获同样很 多，不过这本书编著的时间比较早了，有的概念比较老，还是要有鉴别的去看。</p><p>(4)《Elements Statistic Learning》，(ESL)，经典巨著之一 ESL，看了几章，跟PRML 里面内容有重叠，不过更多是从统计的角度来讲的。<br> (5)《百面机器学习》。由 hulu 工程师编写的一本书，对于常见的机器学习问题做了总结，虽然是应对面试的，但是里面的问题都是工业界常见的问题，我们在实验过程中也会 经常遇见，非常建议读一遍。</p><p>多看论文，多写代码。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020秋招常见视觉面试题整理</title>
    <link href="undefined2020/09/08/mianshi/"/>
    <url>2020/09/08/mianshi/</url>
    
    <content type="html"><![CDATA[<p>2020秋招，牛客上面面试问到的问题整理，太过简单的没有记录，太偏的没有记录，主要涵盖深度学习、机器学习、目标检测</p><p>1.介绍一下faster rcnn</p><p>2.faster rcnn如何进行feature map选取的</p><p>3.faster rcnn中的anchor作用，怎么进行不同box提取</p><p>4.yolo和faster rcnn区别</p><p>5.有哪些激活函数，有什么用，各自的特点，改进了什么？</p><p>6.池化怎么进行反向传播</p><p>7.介绍逻辑回归以及其损失函数，LR推倒</p><p>8.如何解决类别不平衡问题</p><p>9.ResNet的特点</p><p>10.为什么使用1x1卷积？哪些情况下使用1x1卷积？</p><p>11.SVM原理，具有哪些核函数，软间隔是怎么做的</p><p>12.L1正则化和L2正则化区别，为什么L1能够产生稀疏性</p><p>13.如何解决前景背景数量不均衡</p><p>14.mobilenetv1和v2的区别</p><p>15.训练过程中loss一直无法收敛，可能的情况</p><p>16.为什么卷积核都是基数（padding角度）</p><p>17.双线性插值</p><p>18.inception如何减少计算量</p><p>19.反卷积操作</p><p>20.SSD里面的OHEM，正负样本为什么是1:3</p><p>21.SSD缺点，特点，介绍</p><p>22.FPN结构，FPN主要解决什么问题</p><p>23.Inception和resnet区别</p><p>24.细粒度分类用inception还是resnet好，类别不平衡用inception还是resnet好</p><p>25.roi align和roi pooling区别</p><p>26.介绍cascade rcnn</p><p>27.shufflenet</p><p>28.FCOS模型的思想</p><p>29.怎么加快网络的收敛速度</p><p>30.求cross entropy的值</p><p>31.如何解决过拟合</p><p>32.YOLOv3和SSD的致命缺点</p><p>33.检测的框角度偏移如何解决</p><p>34.SSD和Faster RCNN的本质区别</p><p>35.遇见过梯度消失没有，常用框架是怎么判断梯度消失的</p><p>36.PCA，SVD分解的区别与联系</p><p>37.学习率衰减策略</p><p>38.LR和SVM的联系和区别</p><p>39.梯度消失和梯度爆炸出现的原因</p><p>40.介绍一下BCE loss Dice loss， focal loss，triplet loss， center loss</p><p>41.mAP计算</p><p>42.空洞卷积</p><p>43.yolo和SSD的区别</p><p>44.kmeans和GMM区别与联系</p><p>45.python装饰器，python迭代器和生成器</p><p>46.AUC衡量模型的什么能力</p><p>47.各种优化器性能比较</p><p>48.ohem和focal loss的关系</p><p>49.BN在训练期间和测试期间的区别</p><p>50.感受野计算</p><p>51.KKT条件</p><p>52.python动态数组如何实现的</p><p>53.anchor大小选取，长宽比选取</p><p>54.为什么mobilenet、shufflenet在理论上面速度很快，但是工程上并没有特别大的提升</p><p>55.为什么卷积核一般都是奇数</p><p>56.倾斜边界框检测如何计算IOU值</p><p>57.BN前向传播公式，注意train和infer的区别</p><p>58.BN为什么能够加速模型收敛？为什么具有更好的泛化能力</p><p>59.Relu，sigmoid，tanh函数公式，范围，导数公式，范围</p><p>60.为什么采用小卷积核（使用3x3而不是5x5，7x7）</p><p>61.upsample和pooling存在的问题</p><p>62.python的深拷贝和浅拷贝</p><p>63.AUC和F1-score</p><p>64.softmax和sigmoid的区别</p><p>65.NMS算法优化，性能优化</p><p>66.smooth l1损失函数</p><p>67.怎么消除漏检和误检</p><p>68.特征相关性会对模型产生什么影响？如何解决</p><p>69.如何解决梯度消失和梯度爆炸</p><p>70.如何使用pytorch做模型压缩和模型蒸馏，分类和检测方向</p><p>71.使用numpy写一个batchnorm层</p><p>72.简要介绍一下yolo系列</p><p>73.IOU loss，GIOU loss，DIOU loss</p><p>74.简要介绍一下rcnn系列</p><p>75.smooth l1特点</p><p>76.常见优化器各自特点，SGD，Adam</p><p>77.VGG网络及其改进</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>YOLOv4</title>
    <link href="undefined2020/08/20/yolov4/"/>
    <url>2020/08/20/yolov4/</url>
    
    <content type="html"><![CDATA[<p>YOLOv4: Optimal Speed and Accuracy of Object Detection.</p><p>arxiv: <a href="https://arxiv.org/abs/2004.10934" target="_blank" rel="noopener">https://arxiv.org/abs/2004.10934</a></p><p>github: <a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet</a></p><p>第一版YOLO文章的发布，标志着基于深度学习的单阶段目标检测算法的开端，随后相继出现了YOLOv2，YOLOv3等家族系列文章。最近，YOLOv4正式发表，文章的主要内容是结合大量前人的工作，进行了大量的实验，并在此基础上面进行了一定的创新，在COCO数据集上面实现了43.5%的mAP，并且保持了65FPS的速度，实现了速度和精度的平衡</p><p>文章中将前人在目标检测领域的工作大致分为两类：bag of freebies和bag of specials， 前者主要指的是在训练过程中应用的技巧，这种技巧不会显著增强模型测试阶段的速度和模型复杂度负担，只体现在训练阶段，主要就是数据增强操作。后者会稍微增加模型复杂度和时间开销，但是对精度会有更大的提升，例如一些后处理操作(NMS)</p><p>本文主要对YOLOv4中提到的创新进行分析，并对其中集中常见的技巧进行介绍</p><h3 id="Mosaic数据增强"><a href="#Mosaic数据增强" class="headerlink" title="Mosaic数据增强"></a>Mosaic数据增强</h3><p>mosaic数据增强参考了cutmix的方法，不同于一般的数据增强对单张图片进行操作，cutmix将两张图片进行拼接，成为一张图片，具体实现过程是，对图片A随机生成一个裁剪框B，裁剪掉A的相应位置，然后用图片B中的对应位置的ROI去填充A中的对应裁剪位置。对应的图片label是两张图片分别对应的label的线性组合。</p><center><img src="/2020/08/20/yolov4/cutmix.png" srcset="undefined" height="250"></center><p>cutmix采用了两张图片，而本文提出的mosaic方法采用了四张图片，这样做的主要原因是，在目标检测领域，小目标一般要比中目标和打目标检测的难度要大，对应的AP值相对低一些，而且小目标图像分布并不均匀，有的图片中存在，有的并不存在，mosaic方法主要有两个优点：（1）丰富了数据集。随机使用四张图片，然后随机缩放再拼接，丰富了数据集，特别是随机缩放增加了很多小目标，能够获得更好的鲁棒性（2）减少GPU开销。相当于一张图片包含四张图片的信息，这样的话batch size就不需要太大，降低了对于GPU的需求，在一块GPU上面也能够训练</p><center><img src="/2020/08/20/yolov4/mosaic.png" srcset="undefined" height="300"></center>]]></content>
    
    
    
    <tags>
      
      <tag>object detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于深度学习的视频分类</title>
    <link href="undefined2020/07/17/video-class/"/>
    <url>2020/07/17/video-class/</url>
    
    <content type="html"><![CDATA[<p>利用深度学习进行图像分类发展到目前已经日趋成熟，在ImageNet数据集上面的准确率已经超过人类水平，但是视频分类仍然处在一个萌芽时期，视频分类技术并没有像二维图像分类那样成熟 </p><h3 id="视频分类的难点"><a href="#视频分类的难点" class="headerlink" title="视频分类的难点"></a>视频分类的难点</h3><p>视频分类相较于图像分类，主要有两个难点： </p><h5 id="1-如何有效利用时序信息"><a href="#1-如何有效利用时序信息" class="headerlink" title="1.如何有效利用时序信息"></a>1.如何有效利用时序信息</h5><blockquote><p>深度学习利用卷积处理二维图像，能够有效利用图像中物体的位置、角度、色彩等信息，但是视频数据中会多出一维时间信息，而这一维中会蕴含大量有效信息，例如击球场景下，前一帧图像中击球动作发生，后一帧球轨迹改变，这里面就蕴含直接的因果关系，但这是时间维度上面的，传统的二维卷积没有办法学习和利用到这种信息</p></blockquote><h5 id="2-如何减少运算量"><a href="#2-如何减少运算量" class="headerlink" title="2.如何减少运算量"></a>2.如何减少运算量</h5><blockquote><p>可以将视频视为在时间维度上面离散的二维图像，利用深度学习做视频分类最直接的方法就是利用3维卷积，相当于将原来的2维卷积核扩展为3维，这样看似简单的思想，带来的计算量的增加却是非常可怖的，因此如何减少视频分类任务中所需的计算量也是至关重要的一点</p></blockquote><h3 id="主流方法"><a href="#主流方法" class="headerlink" title="主流方法"></a>主流方法</h3><p>目前基于深度学习的视频分类任务主要有两个大的方向：</p><h5 id="1-基于3D卷积的方法"><a href="#1-基于3D卷积的方法" class="headerlink" title="1.基于3D卷积的方法"></a>1.基于3D卷积的方法</h5><blockquote><p>将原有的(a, b, c)大小的卷积核转换为(a, b, c, n)大小，这样能够利用时间维度信息，但是同时带来的是计算量的提升，同时精度也不是很高，因此针对于如何减少计算量和提升精度提出了一系列改进方法，例如18年cvpr的文章《ECO: Efficient convolutional Network For online video understanding》，首先提取离散帧图像，然后利用2D卷积对每一帧图像进行特征提取，然后将提取到的特征进行融合，最后利用一个3D卷积进行分类，相比于同时期的sota模型速度提升8-10倍</p></blockquote><center><img src="/2020/07/17/video-class/3D.png" srcset="undefined"></center><h5 id="2-CRNN"><a href="#2-CRNN" class="headerlink" title="2.CRNN"></a>2.CRNN</h5><blockquote><p>CRNN实际上是CNN+RNN，利用encoder-decoder结构，采用CNN对视频数据进行编码，然后利用RNN进行时序图片信息的解码，具体来讲，采用2D卷积网络将一系列2D视频帧进行编码，得到对应的一维向量，RNN网络(一般采用LSTM)的输入是编码后的一维信息，</p></blockquote><center><img src="/2020/07/17/video-class/crnn.png" srcset="undefined"></center>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>单阶段目标检测算法</title>
    <link href="undefined2020/05/01/one-stage-detection/"/>
    <url>2020/05/01/one-stage-detection/</url>
    
    <content type="html"><![CDATA[<p>上一篇<a href="https://cowarder.site/2019/11/30/two-stage-detection/">文章</a>中，我们详细介绍了几种常见的两阶段目标检测算法，两阶段目标检测算法由于proposal的产生，在精度上面更具优势，但是单阶段目标检测算法在速度上面更快</p><p>本文将整理几种常见的单阶段目标检测算法(YOLO, YOLOv2, YOLOv3, SSD, RetinaNet)，主要从解决的问题、创新点、算法具体流程、仍然存在的问题等方面进行阐述</p><h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a><a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">YOLO</a></h3><h4 id="时间：2015"><a href="#时间：2015" class="headerlink" title="时间：2015"></a>时间：2015</h4><center><img src="/2020/05/01/one-stage-detection/yolo.png" srcset="undefined" height="250"></center><h4 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h4><p>两阶段目标检测中检测速度太慢的问题</p><h4 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h4><p>YOLO是深度学习目标检测领域第一个单阶段目标检测算法，在Pascal VOC2007数据集上面实现了52.7%的mAP，155fps。作者完全舍弃了之前存在的”proposal + classification”的固有检测模式，将检测问题由一个分类问题转换为一个回归问题</p><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p>1.将输入图片resize到$448\times 448$</p><p>2.通过CNN将输入图像分割成$S\times S$的网格，每个网格负责去检测中心点落在其中的目标</p><p>3.每个网格需要预测B个边界框，每个边界框具有位置以及置信度信息，每个网格同时需要预测一个类别信息，最终$S\times S\times(B*5+C)$</p><p>4.S=7, B=2, C=20 (Pascal数据集)</p><h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><p>YOLO中最大的问题在于对于一个cell，我们只能预测一个类别信息，也就意味着一个cell最终只能预测一个物体，如果有两个ground truth的中心点同时落到一个cell中，那么只能保留其中的一个</p><h3 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a><a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">YOLOv2</a></h3><h4 id="时间：2016"><a href="#时间：2016" class="headerlink" title="时间：2016"></a>时间：2016</h4><h4 id="解决的问题-1"><a href="#解决的问题-1" class="headerlink" title="解决的问题"></a>解决的问题</h4><p>在继续保持速度的基础上，从预测更加准确(Better)，速度更快(Faster)， 识别更多对象(Stronger)这三个方面进行了改进。针对YOLO每个cell只能预测一个物体的问题，YOLOv2引入了anchor的概念</p><h4 id="改进策略"><a href="#改进策略" class="headerlink" title="改进策略"></a>改进策略</h4><h5 id="1-BN"><a href="#1-BN" class="headerlink" title="1.BN"></a>1.BN</h5><p>Batch Normalization使得mAP有2.4的提升。引入BN有助于解决反向传播中出现的梯度消失和梯度爆炸的问题，降低对于超参数的敏感性，同时起到了一定的正则化的作用</p><h5 id="2-使用高分辨率图像微调分类模型"><a href="#2-使用高分辨率图像微调分类模型" class="headerlink" title="2.使用高分辨率图像微调分类模型"></a>2.使用高分辨率图像微调分类模型</h5><p>mAP提升了3.7。YOLO采用ImageNet数据集$224\times 224$的样本来预训练CNN，然后在检测任务中，检测样本采用$448\times 448$的分辨率图像，这样会对模型精度产生影响，所以YOLOv2在利用$224\times 224$图像进行分类模型训练之后，再采用$448\times 448$高分辨率图像对模型进行微调(10 epoch)，使得网络特征逐渐适应$448\times 448$分辨率的图像，在利用$448\times 448$的检测样本进行训练，缓解了分辨率带来的差异</p><h5 id="3-引入anchor"><a href="#3-引入anchor" class="headerlink" title="3.引入anchor"></a>3.引入anchor</h5><p>召回率大幅度提升到88%，mAP轻微下降0.2。借鉴Fater RCNN的思想，在模型中引入了anchor的概念。虽然精度有所下降，但是紧接着对anchor进行了改良</p><h5 id="4-聚类提取先验框尺寸"><a href="#4-聚类提取先验框尺寸" class="headerlink" title="4.聚类提取先验框尺寸"></a>4.聚类提取先验框尺寸</h5><p>使得mAP有4.8的提升，这也是YOLOv2中作出的最大改进。之前的先验框尺寸都是人工设置的，YOLOv2尝试使用更加符合样本对象尺寸的先验框，但是聚类时并不是采用的欧式距离，因为欧式距离受到尺度的影响，往往大的边框误差也会相应变大，YOLOv2中采用的是$1-IOU(A, B)$来作为距离</p><h5 id="5-约束预测边界框位置"><a href="#5-约束预测边界框位置" class="headerlink" title="5.约束预测边界框位置"></a>5.约束预测边界框位置</h5><p>借鉴Faster RCNN的思想，训练的早期阶段，位置预测非常不稳定，因此YOLOv2中不是直接预测，而是利用图像尺寸对边界框位置及尺寸进行归一化，预测一个相对值，然后进行边界框回归，进一步调整边界框位置</p><center><img src="/2020/05/01/one-stage-detection/bbr.png" srcset="undefined" height="250"></center><h5 id="6-多尺度训练"><a href="#6-多尺度训练" class="headerlink" title="6.多尺度训练"></a>6.多尺度训练</h5><p>对mAP有1.4的提升。训练的时候每10个batch随机更换一个尺寸，使得图像能够适应各种大小的图像检测</p><h5 id="7-使用更高分辨率的图像输入"><a href="#7-使用更高分辨率的图像输入" class="headerlink" title="7.使用更高分辨率的图像输入"></a>7.使用更高分辨率的图像输入</h5><p>mAP提升1.8。YOLOv2调整之后能够接受各种图像尺寸输入，通常是采用$416\times 416$大小的数据，如果使用$544\times 544$大小的输入，精度会有提升</p><h5 id="8-提出DarkNet-19的网络结构"><a href="#8-提出DarkNet-19的网络结构" class="headerlink" title="8.提出DarkNet-19的网络结构"></a>8.提出DarkNet-19的网络结构</h5><p>DarkNet-19模型要比VGG-16要小，但是精度不弱于VGG-16，浮点运算减少到接近1/5，因此更快</p><h3 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a><a href="https://arxiv.org/abs/1804.02767" target="_blank" rel="noopener">YOLOv3</a></h3><center><img src="/2020/05/01/one-stage-detection/darknet53.png" srcset="undefined" height="300"></center><h4 id="时间：2018"><a href="#时间：2018" class="headerlink" title="时间：2018"></a>时间：2018</h4><h4 id="YOLOv3网络结构"><a href="#YOLOv3网络结构" class="headerlink" title="YOLOv3网络结构"></a>YOLOv3网络结构</h4><p>1.YOLOv3采用了Darknet-53的结构，虽然相较于Darknet-19速度有所降低，但是依然保持高性能，并且精度上面的提升很大。YOLOv3中只有卷积层，通过调节卷积层的步长控制特征图的尺寸。</p><p>2.借鉴了特征金字塔的思想，小尺寸特征用于检测大物体，大尺寸特征用于检测小物体，特征图的输出维度为$N\times N \times [3\times (4+1+80)]$，$N\times N$为输出特征图尺寸，3代表anchor数目，4表示$(t_x, t_y, t_w, t_h)$，1代表置信度，80代表物体类别</p><p>3.YOLOv3一共有三个特征图，分别下采样8倍，16倍，32倍</p><p>4.其中含有两种特征融合的方法。shortcut和route，shortcut就是resnet的思想，直接两层特征相加，route是两层特征在channel维度上面进行拼接</p><h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a><a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">SSD</a></h3><p>SSD是深度学习单阶段目标检测模型的第二个模型，VOC07 mAP=76.8%， VOC12 mAP=74.9%，59fps，</p><h4 id="时间：2015-1"><a href="#时间：2015-1" class="headerlink" title="时间：2015"></a>时间：2015</h4><h4 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h4><p>1.SSD提取了不同尺度的特征图来做检测，大尺度特征图可以用来检测小物体，小尺度特征图可以用来检测大物体</p><p>2.SSD采用不同尺度和长宽比的anchor</p><p>YOLO的缺点是难以检测小物体，定位不准，这两点改进使得SSD在一定程度上面克服了YOLO的缺点</p><h3 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a><a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">RetinaNet</a></h3><center><img src="/2020/05/01/one-stage-detection/focal.png" srcset="undefined" height="250"></center><h4 id="时间：2017"><a href="#时间：2017" class="headerlink" title="时间：2017"></a>时间：2017</h4><h4 id="创新-1"><a href="#创新-1" class="headerlink" title="创新"></a>创新</h4><p>这篇文章的创新点不在于其网络模型结构，而在于其提出了一种解决单阶段目标检测中难易样本分布不均匀问题的方法：Focal loss。</p><p>Focal loss的提出主要是为了解决难易样本分布不均衡的问题，正样本分类为正样本，负样本分类为负样本，这样的都算易样本，而将正样本分类为负样本，负样本分类为正样本的，并且他们的置信度还很高，这种属于难样本</p><center><img src="/2020/05/01/one-stage-detection/fl1.png" srcset="undefined"></center><p>为了解决正负样本分布不均，我们可以加上一个alpha参数对其进行加权</p><center><img src="/2020/05/01/one-stage-detection/fl2.png" srcset="undefined"></center><p>但是这并不能解决问题，为了进一步解决难易样本分布不均的问题，提出了focal loss，本质上是将高置信度的样本损失再降低一些，低置信度样本损失增加一些</p><center><img src="/2020/05/01/one-stage-detection/fl3.png" srcset="undefined"></center><p>综合两者，最终的Focal loss形式</p><center><img src="/2020/05/01/one-stage-detection/fl4.png" srcset="undefined"></center>]]></content>
    
    
    
    <tags>
      
      <tag>object detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>小目标检测(Small Object Detection)</title>
    <link href="undefined2020/04/15/small-od/"/>
    <url>2020/04/15/small-od/</url>
    
    <content type="html"><![CDATA[<p>现今存在的通用目标检测方法大多数在一般大小的物体上面检测效果较好，但是针对小物体的检测效果较差。目前的检测方法基本基于深度学习，而神经网络是由逐层卷积网络组成的，我们以一个常用的分类神经网络举例。</p><center><img src="/2020/04/15/small-od/odline.png" srcset="undefined" height="300">  <div>    Generic image classification baseline.  </div></center><p>网络中包含有一系列的卷积+pooling层，在检测网络中，如YOLO、SSD、Faster R-CNN等中也会大量使用这种类似结构，我们利用神经网络将600x600的输入转化为30x30的特征。但是这样的网络结果造成的一个非常直观的问题，如果在原始(600x600)图片中很小的物体，经过卷积映射为低维特征(30x30)之后，小物体特征几乎可以忽略不计，甚至完全消失，在训练过程中，网络在靠后的特征层中就失去了小物体的监督信息，因此就会造成检测性能的下降。</p><p>本文旨在总结几种常见的解决小物体检测的方法，大体可以分为四类：多尺度特征学习、数据增强、采用训练策略、基于GAN的检测。</p><h4 id="1-多尺度特征学习"><a href="#1-多尺度特征学习" class="headerlink" title="1.多尺度特征学习"></a>1.多尺度特征学习</h4><p>如何去处理特征尺度问题对于小物体检测十分重要，大体可以分为7类：特征图像金字塔、单尺度特征、金字塔特征层、整合特征、特征金字塔网络、特征融合和特征金字塔生成、多尺度融合模块。</p><h5 id="特征图像金字塔"><a href="#特征图像金字塔" class="headerlink" title="特征图像金字塔"></a>特征图像金字塔</h5><center><img src="/2020/04/15/small-od/a.png" srcset="undefined" height="230"></center><p>将输入图片resize到不同尺度，在不同尺度上面进行训练，学习得到在几个不同尺度上面的检测器，利用不同的检测器对图片作出检测。</p><h5 id="单尺度特征"><a href="#单尺度特征" class="headerlink" title="单尺度特征"></a>单尺度特征</h5><center><img src="/2020/04/15/small-od/b.png" srcset="undefined" height="150"></center><p>例如Faster RCNN等方法，只采用单尺度特征层进行检测，这样的话由于最后一层的感受野是固定的，因此实际上是忽略了对于小物体检测的关注。</p><h5 id="金字塔特征图"><a href="#金字塔特征图" class="headerlink" title="金字塔特征图"></a>金字塔特征图</h5><p>对于特征图像金字塔的改进策略就是不是在不同尺度的图片上面进行检测，而是在图片的不同特征层上面进行检测，例如常见的SSD网络。</p><center><img src="/2020/04/15/small-od/c.png" srcset="undefined" height="150"></center><h5 id="整合特征"><a href="#整合特征" class="headerlink" title="整合特征"></a>整合特征</h5><p>首先我们对图片提取不同层次的特征，然后将来自不同层次的特征映射进行一个融合，形成一个融合特征，然后在融合特征上面进行检测。</p><center><img src="/2020/04/15/small-od/d.png" srcset="undefined" height="200"></center><h5 id="特征金字塔网络"><a href="#特征金字塔网络" class="headerlink" title="特征金字塔网络"></a>特征金字塔网络</h5><p>特征自底向上传播，然后自顶向下上采样，对应层进行融合，分别在不同的层作出预测。</p><center><img src="/2020/04/15/small-od/e.png" srcset="undefined" height="200"></center><h5 id="特征融合和特征金字塔生成"><a href="#特征融合和特征金字塔生成" class="headerlink" title="特征融合和特征金字塔生成"></a>特征融合和特征金字塔生成</h5><p>首先将来自不同尺度的特征进行级连融合在一起生成特征图，然后利用生成的特征图产生一系列的金字塔特征，分别在每一层作出检测。</p><center><img src="/2020/04/15/small-od/f.png" srcset="undefined" height="200"></center><h5 id="多尺度融合模块"><a href="#多尺度融合模块" class="headerlink" title="多尺度融合模块"></a>多尺度融合模块</h5><p>采用skip-connection将不同尺度特征进行融合。</p><center><img src="/2020/04/15/small-od/g.png" srcset="undefined" height="200"></center><h4 id="2-数据增强"><a href="#2-数据增强" class="headerlink" title="2.数据增强"></a>2.数据增强</h4><p>采用例如flipping、cropping、rotating、contrasting等手段对原始图片进行变换，实际上是增加了数据的多样性，这对改进训练效果是十分有效的。特别来讲，将那些包含小物体的图片进行过采样，能够有效改善小物体检测的性能。</p><p><a href="https://arxiv.org/abs/1902.07296" target="_blank" rel="noopener">Augmentation for small object detection</a></p><h4 id="3-训练策略"><a href="#3-训练策略" class="headerlink" title="3.训练策略"></a>3.训练策略</h4><p><a href="https://arxiv.org/abs/1711.08189" target="_blank" rel="noopener">An Analysis of Scale Invariance in Object Detection - SNIP</a>一文针对小物体检测，在训练过程中进行了改进，SNIP是对多尺度训练的一种扩展。具体方法是</p><p>1.选取三种图像分辨率训练得到的proposal。</p><p>2.对于每个分辨率的图像，BP时只回传在对应尺度范围内的proposal梯度。</p><p>3.这样保证了只使用一个网络，但是每次训练的物体的尺寸都是一致的。</p><p>包括SNIP的改进<a href="https://arxiv.org/abs/1805.09300" target="_blank" rel="noopener">SNIPER</a>都是对多尺度训练作出的改进。</p><h4 id="基于GAN的检测"><a href="#基于GAN的检测" class="headerlink" title="基于GAN的检测"></a>基于GAN的检测</h4><p><a href="https://arxiv.org/abs/1706.05274" target="_blank" rel="noopener">Perceptual GAN</a>中对生成器和判别器都进行了修改，使得生成器能够生成小目标的超分表达，判别器分为两个分枝，对抗分枝和感知分枝。对抗分枝用于区分生成的超分表达与大ground truth图片。感知分枝用于验证生成的表达对于检测精度的获益量。</p><p><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yongqiang_Zhang_SOD-MTGAN_Small_Object_ECCV_2018_paper.pdf" target="_blank" rel="noopener">MTGAN</a>方法中采用GAN的思想，Generator重建低分辨率RoI图像至高分辨率，Discriminator是multi-task的，不仅仅知道G重建出更高质量的高分辨率RoI图像，并且基于重建后的高分辨率目标图像，同时完成bbox cls、reg的任务。</p><h4 id="延伸文章"><a href="#延伸文章" class="headerlink" title="延伸文章"></a>延伸文章</h4><p><a href="https://arxiv.org/abs/1904.09730v1" target="_blank" rel="noopener">An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</a></p><p><a href="https://arxiv.org/abs/2003.09085v5" target="_blank" rel="noopener">Small-Object Detection in Remote Sensing Images with End-to-End Edge-Enhanced GAN and Object Detector Network</a></p><p><a href="https://arxiv.org/abs/2005.11552v1" target="_blank" rel="noopener">Underwater object detection using Invert Multi-Class Adaboost with deep learning</a></p><p><a href="https://arxiv.org/abs/1805.07009v3" target="_blank" rel="noopener">MDSSD: Multi-scale Deconvolutional Single Shot Detector for Small Objects</a></p><p><a href="https://arxiv.org/abs/1709.05054v3" target="_blank" rel="noopener">Feature-Fused SSD: Fast Detection for Small Objects</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>object detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Batch Normalization</title>
    <link href="undefined2020/03/04/batch-norm/"/>
    <url>2020/03/04/batch-norm/</url>
    
    <content type="html"><![CDATA[<p>这篇文章来回顾一下深度学习网络模型中常见的操作:Batch Normalization。<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Batch Normalization</a>是在2015年由Szegedy提出的。主要是解决神经网络中存在的Interval Covariate Shift问题。除了回顾batch normalization有关原理，本文也会介绍几种常见的对于Batch Normalization的改进。</p><h4 id="Interval-Covariate-Shift"><a href="#Interval-Covariate-Shift" class="headerlink" title="Interval Covariate Shift"></a>Interval Covariate Shift</h4><p>在Batch Normalization出现之前，训练一个层数较多的神经网络模型是十分困难的，经常出现不收敛的情况，其中重要的原因是，神经网络涉及到很多层的叠加，每一层的参数变化会导致上层输出的改变，经过层层叠加，底层即便是一个微小的波动，也会导致上层输入特征的巨大变化，这就使得上层参数需要不断改变去适应由底层传来的特征，为了使得模型能够收敛，我们需要更加仔细地选择学习率、权重初始化、尽可能选择更好的参数更新策略。</p><p>在统计机器学习中一个经典假设是源空间(source domain)和目标空间(target domain)的数据分布是一致的。而covariate shift就是分布不一致假设之下的一个分支问题。概括来讲，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布与各层输入信号分布不同，而且会随着网络深度的增加而增大，这就是interval covariate shift的含义。</p><h4 id="BN的具体流程"><a href="#BN的具体流程" class="headerlink" title="BN的具体流程"></a>BN的具体流程</h4><center><img src="/2020/03/04/batch-norm/bn.png" srcset="undefined" width="400"></center>假设我们每个batch有m个样本，首先我们需要计算m个样本的均值和方差，利用均值和方差进行标准化后，再利用$\gamma$和$\beta$进行缩放和平移，如果仅仅是对每一层进行标准化的话，可能会降低每层网络的表达能力，以sigmoid函数为例，sigmoid函数分为饱和区和非饱和区，而标准化之后的数据会将数据映射到非饱和区，使得其仅仅具有线性表达能力，因此BN中加入了平移和缩放操作来缓解这个问题。<pre><code class="python">import numpy as npdef Batchnorm(x, gamma, beta, bn_param):    # x_shape:[B, C, H, W]    running_mean = bn_param[&#39;running_mean&#39;]    running_var = bn_param[&#39;running_var&#39;]    results = 0.    eps = 1e-5    x_mean = np.mean(x, axis=(0, 2, 3), keepdims=True)    x_var = np.var(x, axis=(0, 2, 3), keepdims=True0)    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)    results = gamma * x_normalized + beta    # 因为在测试时是单个图片测试，这里保留训练时的均值和方差，用在后面测试时用    running_mean = momentum * running_mean + (1 - momentum) * x_mean    running_var = momentum * running_var + (1 - momentum) * x_var    bn_param[&#39;running_mean&#39;] = running_mean    bn_param[&#39;running_var&#39;] = running_var    return results, bn_param</code></pre><h4 id="BN的缺陷"><a href="#BN的缺陷" class="headerlink" title="BN的缺陷"></a>BN的缺陷</h4><p>BN依赖于batch大小的选择，如果太小会造成模型的不稳定，实验表明，ResNet在ImageNet数据集上进行训练，batch size从16降到8之后性能会有明显的下降，因此BN在以下两种情况下不适应。</p><p>1.batch非常小。有时受限于计算资源，不得不选择一个较小的batch size，这时使用BN可能不会造成性能提升。</p><p>2.RNN。由于RNN是一个动态的网络结构，同一个batch中的样本长短不一，因此每个batch的统计量不稳定，使得BN无法正确使用。</p><h4 id="BN的改进"><a href="#BN的改进" class="headerlink" title="BN的改进"></a>BN的改进</h4><h5 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a><a href="https://arxiv.org/pdf/1607.06450v1.pdf" target="_blank" rel="noopener">Layer Normalization</a></h5><p>Layer Normalization就是针对BN的不足提出的，BN可以看作是一种纵向的归一化，而LN可以看作是一种横向的归一化，LN针对单个训练样本进行训练，不依赖于其他数据，因此可以避免BN受batch size影响的情况出现，可以应用于小batch数据上面。</p><center><img src="/2020/03/04/batch-norm/layer.png" srcset="undefined"></center><p>对于四维图像数据(B, C, H, W)来说，BN进行归一化的时候使用的均值和方差形状都为(1, C, 1, 1)，也就是说每一个channel保留一个值，在B、H、W上面求均值。而对于LN来讲，均值和方差形状为(B, 1, 1, 1)，即batch中的每个样本都有一个固定的值来对其进行归一化，在C、H、W、上面求均值。</p><pre><code class="python">def Layernorm(x, gamma, beta):    # x_shape:[B, C, H, W]    results = 0.    eps = 1e-5    x_mean = np.mean(x, axis=(1, 2, 3), keepdims=True)    x_var = np.var(x, axis=(1, 2, 3), keepdims=True0)    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)    results = gamma * x_normalized + beta    return results</code></pre><h5 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a><a href="https://arxiv.org/pdf/1607.08022.pdf" target="_blank" rel="noopener">Instance Normalization</a></h5><p>IN最初应用于图像的风格迁移，作者发现，在生成模型中，feature map的各个channel的均值和方差会影响最终生成图像的风格，因此需要对HW进行归一化，加速模型收敛，并且保持每个样本之间的独立性，IN同样也是不受batch size影响的，均值和方差为(B, C, 1, 1)</p><pre><code class="python">def Instancenorm(x, gamma, beta):    # x_shape:[B, C, H, W]    results = 0.    eps = 1e-5    x_mean = np.mean(x, axis=(2, 3), keepdims=True)    x_var = np.var(x, axis=(2, 3), keepdims=True0)    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)    results = gamma * x_normalized + beta    return results</code></pre><h5 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a><a href="https://arxiv.org/pdf/1803.08494.pdf" target="_blank" rel="noopener">Group Normalization</a></h5><center><img src="/2020/03/04/batch-norm/gn.png" srcset="undefined"></center><p>主要是针对BN中小batch效果差的问题，GN将channel方向分group，每个group内做归一化，计算(C//G)*H*W的均值，同样的，GN与batch size无关，不受约束。</p><pre><code class="python">def GroupNorm(x, gamma, beta, G=16):    # x_shape:[B, C, H, W]    results = 0.    eps = 1e-5    x = np.reshape(x, (x.shape[0], G, x.shape[1]/16, x.shape[2], x.shape[3]))    x_mean = np.mean(x, axis=(2, 3, 4), keepdims=True)    x_var = np.var(x, axis=(2, 3, 4), keepdims=True0)    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)    results = gamma * x_normalized + beta    return results</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Poly-YOLO</title>
    <link href="undefined2020/02/25/poly-yolo/"/>
    <url>2020/02/25/poly-yolo/</url>
    
    <content type="html"><![CDATA[<p>论文阅读：<a href="https://arxiv.org/abs/2005.13243" target="_blank" rel="noopener">POLY-YOLO: HIGHER SPEED, MORE PRECISE DETECTION AND INSTANCE SEGMENTATION FOR YOLOV3</a></p><h3 id="什么是Poly-YOLO"><a href="#什么是Poly-YOLO" class="headerlink" title="什么是Poly-YOLO"></a>什么是Poly-YOLO</h3><p>简言之，Poly-YOLO是YOLOv3的一个改进版本，产生的实际效果与Mask-RCNN相似，但是Poly-YOLO更快。文章作者声称Poly-YOLO相较于YOLOv3速度更快，同时mAP提升40%。Poly-YOLO的轻量级版本-Poly-YOLO lite，与YOLOv3具有相似的精度，但是模型大小仅为三分之一，而速度为两倍，因此更加适应于嵌入式设备。<a href="https://gitlab.com/irafm-ai/poly-yolo" target="_blank" rel="noopener">代码</a></p><h3 id="YOLOv3中存在的问题"><a href="#YOLOv3中存在的问题" class="headerlink" title="YOLOv3中存在的问题"></a>YOLOv3中存在的问题</h3><p>本文从分析YOLOv3论文中存在的两个问题，进而进行改进。</p><p>（问题1）标签重写(label rewriting)。在预处理阶段，经常出现label被另外一个label重写的情况，这样就会导致一些正样本没有标签，模型也就不会被训练去检测它们，因此会对性能产生很大影响。以YOLOv3为例，其有三个scale，分别对应检测大、中、小尺寸的物体，每个scale的feature map被划分为s*s的grid，如果两个ground truth的中心落在同一个scale中grid的一个cell，这样就会出现label rewriting的问题。实际上这是YOLOv1中出现的问题，YOLOv3中引入了anchor的概念，每个cell预测三个不同尺寸的anchor，原文应该表述为：如果两个ground truth的中心落在同一个scale中grid的一个cell，并且由同一个size的anchor负责预测，这样就会出现label writing的问题，详细可参考YOLOv3中Darknet YOLO网络层计算损失函数的<a href="https://github.com/cowarder/yolov3_voc/blob/c321b6b3bf6ab3f737ecf106ca3931b588e84adb/Darknet.py#L35" target="_blank" rel="noopener">过程</a></p><p>（问题2）锚分布(anchor distribution)。YOLOv3对三个不同的scale的feature map分别设定了三个不同尺寸的anchor，高层特征anchor较大，感受野较大，负责预测大物体，底层特征anchor较小，感受野较小，负责预测小物体。每一个ground truth根据其尺寸选择对应的scale的anchor，但是这样有一个前提假设，就是anchor大小的分布需要符合正态分布，而对于大多数都是小物体的数据集，或者大多数都是大物体的数据集，所有的预测结果都会集中在其中的某一层，另外两层就会underused，</p><h3 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h3><p>问题1:问题1可以通过增加feature map缩放系数s来解决，s增加意味着grid中cell数目的增加，这样的话两个ground truth落在同一个cell中的概率就会变得很小，理想情况下s=1，及每个像素点代表一个cell，但是考虑到计算量的问题，肯定不能取1，只能找一个小于1的值。</p><p>问题2:可以采用两种策略解决，第一种是为三个尺度划分感受野，利用两个阈值分隔它们，然后利用k-means根据阈值计算聚类中心，缺点是将数据驱动问题转换为问题驱动问题，只能在固定的尺度上面检测对应的目标，而不是全尺度，这样会浪费网络。第二种是创建一种新的结构，只有一个输出，但是同时融合了三个尺度的特征，这样融合后的结果同时处理所有尺寸的anchor，这样anchor的尺寸仍然是数据驱动的。</p><h3 id="Poly-YOLO结构"><a href="#Poly-YOLO结构" class="headerlink" title="Poly-YOLO结构"></a>Poly-YOLO结构</h3><p>原来的FPN结构被替换为hypercolumn结构，融合不同层次的特征，作者进一步改进，将hypercolumn结构改进为stairstep技术，标准的hypercolumn输入所有层的特征，将他们resize为同样大小进行融合。而stairstep首先将最小的upsample为次小的，两者进行融合为新的最小特征，重复过程，最终得到融合后的特征图。这样并没有增加计算负担，但是在精度上面却有所提高。</p><center><img src="/2020/02/25/poly-yolo/stairstep.png" srcset="undefined" height="200">  <div>    Illustration of the standard hypercolumn scheme (left) and the hypercolumn with stairstep (right).  </div></center><p>Poly-YOLO于YOLOv3结构上面的对比：</p><center><img src="/2020/02/25/poly-yolo/structure.png" srcset="undefined" height="300">  <div>    Comparison of YOLOv3 and Poly-YOLO architectures  </div></center><p>Poly-YOLO在原有的backbone上面也有所改进，将原来的Darknet53改进为SE-Darknet-53，取得了更高的精度，作者减少了backbone中filter的数目，因此Poly-YOLO相较于YOLOv3具有更好的速度，即便输出feature map是原有的四倍。</p><h3 id="多边形边框"><a href="#多边形边框" class="headerlink" title="多边形边框"></a>多边形边框</h3><p>Poly-YOLO通过灵活定义一定数量的点来定义物体的多边形框，多边形框的点，顶点在归一化的极坐标空间中进行处理，这样使得网络不依赖于对象大小的一般形状进行训练。</p><center><img src="/2020/02/25/poly-yolo/poly.png" srcset="undefined" height="200"></center>]]></content>
    
    
    
    <tags>
      
      <tag>论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cvpr2020</title>
    <link href="undefined2020/02/11/cvpr2020/"/>
    <url>2020/02/11/cvpr2020/</url>
    
    <content type="html"><![CDATA[<p>这篇文章主要对CVPR2020中涉及到的目标检测的文章做一个简短总结，提炼总体文章思路，以备查阅。</p><p><a href="https://arxiv.org/abs/1908.01998" target="_blank" rel="noopener">《Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector》</a></p><p>小样本学习的文章。目标检测需要大量标注工作，利用少量标注样本训练模型得到更好的效果在一些任务上面更加需要。文章中主要有两点贡献，首先作者提出了一个新的网络（带有注意力机制的RPN和多关系检测器的小样本目标检测网络），在不用重新训练和微调的情况下检测新的物体，第二点贡献在于设计了一个新的数据集(FSOD)，其中包括1000个类别，每个类别只有很少的图片，利用这个数据集，模型相较于COCO数据集上面进行训练，取得了更好的效果。在few-shot learning任务中，我们一般将带有标签的图片称为support image，等待分割的图片称为query image，注意力机制RPN通过关注给定的支持类别来过滤掉其他类别的对象建议，然后多关系检测器将查询建议与支持图像进行匹配。</p><p><a href="https://arxiv.org/abs/1912.02424" target="_blank" rel="noopener">《Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection》</a></p><p>论文中指出one-stage基于anchor的方法和center-based anchor free的方法之间检测性能的差异主要是来源于正负样本的选择，如果在实验过程中使用相同的正负样本，则两者之间的性能差异不会太大。基于此提出ATSS(Adaptive Training Sample Selection)方法，根据GT的相关统计特征选择合适的anchor box作为正样本，在不增加额外计算量的情况下大幅度提升模型性能。文中选取了anchor-based方法RetinaNet和anchor-free方法FCOS进行了对比，原本的RetinaNet性能要低于    FCOS，但是考虑到他们之间定义正负样本的差异性，RetinaNet使用IOU阈值来区分正负anchor box，处于中间的全部忽略。FCOS使用空间尺寸和尺寸限制来区分正负anchor point，首先需要在GT里面，然后对尺寸做了限制。通过交叉实验，发现在相同正负样本定义下的RetinaNat和FCOS性能几乎一样。</p><p><a href="https://arxiv.org/abs/1912.05384" target="_blank" rel="noopener">《AugFPN: Improving Multi-scale Feature Learning for Object Detection》</a></p><p>文章将Faster RCNN的FPN结构改成AugFPN，在ResNet50和Mobilenetv2上面都有所提升。FPN中，底层特征相对比较大，为了节省计算量通道数相对较少，而高层特征包含较多的语义特征，通道数比较多，在做不同层次的特征融合的时候，高层特征需要利用1x1卷积降低通道数，然后通过两倍上采样和首先在文章中分析了现有的FPN存在的三点缺陷：</p><p>1.before feature fusion。FPN在进行特征融合之前，需要将高层特征利用卷积降低通道数，通道数相同后才能相加，但是不同stage学习到的特征感受野是不一样的，包含的语义信息也不同，直接相加会造成多尺度的表达能力</p><p>2.Top-down feature fusion。将底层特征和高层特征进行融合的过程，虽然底层特征得到了来自高层特征的加强，但是高层特征通过1x1卷积降低通道数，势必造成信息的损失。</p><p>3.After feature fusion。FPN中每个候选区域ROI都是根据起尺度来选择对应的特征层次的，最终每个ROI都只会对应一层特征，而那些被忽略的层也会对结果产生影响，但是信息直接被忽略掉了。</p><ol><li><p>针对上述问题，提出了三点对应的解决方案：</p></li></ol><p>1.consistent supervision。将每个ROI都映射到所有的feature map上面得到对应的feature map，然后对这些feature map做分类和回归，得到损失，将这一损失和网络本身的损失做一个加权求和。</p><p>2.Residual Feature Augmentation。FPN特征融合之前需要做1x1的降维，这会导致高层特征的信息缺失，直接加没有缺失的高层特征。</p><p>3.Soft ROI Selection。FPN重的ROI是根据尺度大小选择对应的特征层，一般小的ROI对应底层特征，大的ROI对应高层特征，但是虽然某个ROI被分到某一层，但是其他特征层同样包含对应的特征信息，作者将ROI对应的所有特征层的特征都取出来，然后利用网络权重参数做一个加权。</p>]]></content>
    
    
    
    <tags>
      
      <tag>object detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MobileNet</title>
    <link href="undefined2020/01/28/mobile/"/>
    <url>2020/01/28/mobile/</url>
    
    <content type="html"><![CDATA[<p>卷积神经网络在计算机视觉领域已经得到了广泛应用，在各种视觉任务中都取得了比较好的效果。大趋势是通过不断增加网络深度、设计更加复杂的网络来提高模型精度。但是随着网络模型的增加，模型存储需要的内存以及运行需要的算力也在不断增加。在某些实际应用领域，例如嵌入式设备，这样庞大而复杂的网络是很难被应用的，一方面是模型过于庞大，面临内存不足的问题，而另外一方面，这些应用领域要求延迟低，而网络参数越多计算量越大，延迟就会越高。因此研究小而且高效的网络对工业界实际应用具有非常大的意义。</p><p>考虑到硬件设备的限制，目前在网络研究方面主要分为两个方面，一是对于训练好的模型进行压缩，减少参数量。另外一种是直接设计一个小的模型进行训练。其目标在于在保证模型精度的情况下减少参数数目，同时提升模型精度。本文所做的改进属于后者。</p><h3 id="深度可分离卷积-depthwise-seperable-convolution"><a href="#深度可分离卷积-depthwise-seperable-convolution" class="headerlink" title="深度可分离卷积(depthwise seperable convolution)"></a>深度可分离卷积(depthwise seperable convolution)</h3><p>MobileNet中的基本组建是深度可分离卷积(depthwise seperable convolution)，首先我们需要分析一个传统的卷积操作。</p><center><img src="/2020/01/28/mobile/conv.png" srcset="undefined" height="200"> </center><p>假设我们有一个12<em>12的输入特征图，通道数为3($D_{in} \times D_{in} \times C_{in}$)，通过256个5\</em>5*3($D_k \times D_k \times C_{in} $)的卷积核进行操作，我们得到了8*8*256的输出特征图($D_{out} \times D_{out} \times C_{out}$)。其中，输出特征图需要有多少个通道数，就需要有对应数目的卷积核，每个卷积核在输入特征图上面遍历操作一遍之后，就得到了一个通道的输出特征图。</p><p>标准卷积的参数量：</p><p>$$Param = D_k \times D_k \times C_{in} \times C_{out}$$</p><p>标准卷积的计算量：</p><p>卷积核与输入卷积特征对应相乘一次，得到输出特征图一个通道上面的一个数值，因此总的计算量为</p><p>$$Computation = D_k \times D_k \times C_{in} \times C_{out} \times D_{out} \times D_{out}$$</p><p>上述过程为不考虑偏差条件下一个卷积的工作过程，而在本文中提出的深度可分离卷积可以分为两个部分：深度卷积(depthwise convolution)和逐点卷积(pointwise convolution)。</p><h4 id="深度卷积"><a href="#深度卷积" class="headerlink" title="深度卷积"></a>深度卷积</h4><p>首先来看深度卷积，我们不再是使用一个5*5*3的卷积核，而是采用三个5*5*1的卷积核，得到8*8*3的特征图。</p><center><img src="/2020/01/28/mobile/depth.png" srcset="undefined" height="200"> </center><p>深度卷积的参数量：</p><p>$$Param = D_k \times D_k \times C_{in}$$</p><p>深度卷积的计算量：</p><p>$$Computation=D_k \times D_k \times C_{in} \times D_{out} \times D_{out}$$</p><p>如果仅仅采用深度卷积，我们可以看到输出特征图的通道数与输入特征图保持一致，这样就会导致特征维度太少，难以获得足够的有效信息，因此需要添加逐点卷积。</p><h4 id="逐点卷积"><a href="#逐点卷积" class="headerlink" title="逐点卷积"></a>逐点卷积</h4><p>逐点卷积就是卷积核大小为1*1的卷积，主要作用就是对特征图进行升维和降维。</p><center><img src="/2020/01/28/mobile/point.png" srcset="undefined" height="200"> </center><p>我们采用256个3通道的1*1卷积操作之后，最终得到了与标准卷积操作后同样大小的8*8*256特征图。</p><p>逐点卷积的参数量：</p><p>$$Param = C_{in}\times C_{out}$$</p><p>逐点卷积的计算量：</p><p>$$Computation = C_{in} \times C_{out}\times D_{out} \times D_{out}$$</p><h4 id="卷积的对比"><a href="#卷积的对比" class="headerlink" title="卷积的对比"></a>卷积的对比</h4><p>深度可分离卷积参数数目：</p><p>$$Param = D_k \times D_k \times C_{in} + C_{in}\times C_{out}$$</p><p>深度可分离卷积的计算量：</p><p>$$Computation = D_k \times D_k \times C_{in} \times D_{out}\times D_{out} + C_{in}\times C_{out}\times D_{out}\times D_{out}$$</p><p>参数量对比：</p><p>$$\frac{D_k \times D_k \times C_{in} + C_{in}\times C_{out}}{D_k \times D_k \times C_{in} \times C_{out}} =\frac{1}{C_{out}}+\frac{1}{D_k^2}$$</p><p>计算量对比：</p><p>$$\frac{D_k \times D_k \times C_{in} \times D_{out}\times D_{out} + C_{in}\times C_{out}\times D_{out}\times D_{out}}{D_k \times D_k \times C_{in} \times C_{out} \times D_{out} \times D_{out}}=\frac{1}{C_{out}}+\frac{1}{D_k^2}$$</p><p>可以看到，深度可分离卷积相较于传统卷积，参数量和计算量均下降为原来的$\frac{1}{C_{out}}+\frac{1}{D_k^2}$</p><h3 id="MobileNet网络"><a href="#MobileNet网络" class="headerlink" title="MobileNet网络"></a>MobileNet网络</h3><p>有了深度可分离卷积作为基本组件，就可以构建网络。</p><center><img src="/2020/01/28/mobile/structure.png" srcset="undefined" height="200"> </center><p>左侧表示的是带有batch normalization和ReLU激活函数的标准卷积层，右侧表示的是MobileNet中的深度可分离卷积。</p><p>MobileNet整体网络结构一共有28层，其中13层是深度可分离卷积。</p><center><img src="/2020/01/28/mobile/full_struc.png" srcset="undefined" height="400"> </center><p>作者在实验部分做了一个参数统计，可以看到我们的参数主要集中在1*1卷积上面。</p><center><img src="/2020/01/28/mobile/param.png" srcset="undefined" height="200"> </center><p>作者将MobileNet与GoogleNet和VGG16进行了对比，可以看到，虽然准确率上面相较于VGG16有所下降，但是计算量和参数量不是一个量级的，而且精度要优于GoogleNet。</p><p><strong>参考资料</strong></p><p>1.<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></p><p>2.<a href="https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69" target="_blank" rel="noopener">Review: MobileNetV1 — Depthwise Separable Convolution (Light Weight Model)</a></p><p>3.<a href="https://zhuanlan.zhihu.com/p/70703846" target="_blank" rel="noopener">轻量级神经网络“巡礼”（二）—— MobileNet，从V1到V3</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读《Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification》</title>
    <link href="undefined2020/01/15/activation/"/>
    <url>2020/01/15/activation/</url>
    
    <content type="html"><![CDATA[<p>最近抽空看了一下何凯明2015年的文章<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">《Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification》</a>，这篇文章里面主要分为两个部分，第一部分作者提出了PReLU激活函数，第二部分提出了Kaiming参数初始化方法，关于Kaiming初始化，我们已经在上一篇文章中做了推倒，这里不再赘述。PReLU方法为ReLU方法的改进版本，因此，本文不仅仅局限于这篇文章，而是借此对几种常见的激活函数进行探讨</p><h3 id="Sigmoid函数和Tanh函数"><a href="#Sigmoid函数和Tanh函数" class="headerlink" title="Sigmoid函数和Tanh函数"></a>Sigmoid函数和Tanh函数</h3><p>在神经网络发展初期，Sigmoid函数和Tanh函数是最常见的激活函数，它们的函数图像如下图所示：</p><center><img src="/2020/01/15/activation/sigtan.png" srcset="undefined"></center><p>其中蓝色的线表示Tanh函数，绿色的线表示Sigmoid函数，可以看到，Tanh激活函数实际上相当于Sigmoid函数的平移</p><p>$$Sigmoid:f(z)=\frac{1}{1+e^{-z}}$$</p><p>$$Tanh:f(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$</p><p>但是在研究过中发现，Sigmoid函数和Tanh函数作激活函数都会导致梯度消失的情况出现，这是因为两者的导数在z取很大或者很小的时候，都会趋于0，导致梯度为0因此出现了梯度消失的情况。</p><h3 id="ReLU函数及其改进"><a href="#ReLU函数及其改进" class="headerlink" title="ReLU函数及其改进"></a>ReLU函数及其改进</h3><p>由于Sigmoid函数和Tanh函数存在梯度消失的问题，因此ReLU函数应运而生：</p><center><img src="/2020/01/15/activation/relu.png" srcset="undefined" width="300" "></center><p>$$ReLU:f(z)=maz(0,z)$$</p><p>ReLU函数主要有三个优点：</p><blockquote><p>1.减少了计算量。Sigmoid和Tanh函数都需要计算指数函数，计算复杂度高，但是ReLU只需要一个阈值即可</p><p>2.ReLU可以有效的解决梯度消失的问题</p><p>3.ReLU的单边抑制提供了网络的稀疏表达能力</p></blockquote><p>但是同样的，ReLU也有其局限性，由于负梯度在经过ReLU单元时被置为0，因此可能出现神经元死亡的问题，实际训练过程中，如果学习率设置的过大，会导致一定比例的神经元不可逆死亡，进而参数梯度无法更新，训练失效。</p><p>为了解决ReLU存在的神经元死亡的问题，提出了Leaky ReLU，Leaky ReLU负半轴是一个斜率为a的线形导数，一般来讲是一个比较小的数值，这样既可以保证单边抑制，又可以保证负半轴梯度不至于完全丢失，但是这个a值需要人为去调整，比较难以选择</p><center><img src="/2020/01/15/activation/leaky.png" srcset="undefined" width="500" "></center><p>在何凯明的这篇文章中提出了PReLU的概念，因为在实际实验中发现，ReLU和Leaky ReLU之间的性能差异非常微小，所以考虑到可能是a值设定的问题，PReLU让网络自己去学习a的取值，将a添加到反向传播过程中，实际应用过程中只需要计算损失函数关于a的导数即可，而且作者让不同channel共享参数，这样在计算过程中，每一层只添加了一个额外的参数，对计算量的影响可以忽略不计</p>]]></content>
    
    
    
    <tags>
      
      <tag>论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>参数初始化</title>
    <link href="undefined2020/01/07/param-init/"/>
    <url>2020/01/07/param-init/</url>
    
    <content type="html"><![CDATA[<p>深度学习中的网络初始化指的是在训练网络之前，对每个节点的权重和偏置进行初始化的过程，一个参数的初始化关系到网络是否能够训练出好的结果以及模型收敛的速度。本文简要介绍两种常见的参数初始化方法的推导过程及原理。</p><h3 id="Xavier初始化"><a href="#Xavier初始化" class="headerlink" title="Xavier初始化"></a>Xavier初始化</h3><p>神经网络做的任务无非是将原本的样本稳定映射为它的类别，也就是将样本空间映射为类别空间，如果样本空间和类别空间差距过大，例如类别空间特别稠密，样本空间特别稀疏，那么在类别空间得到的用于反向传播的误差丢给样本空间之后就会变得微不足道，反之，如果类别空间特别稀疏，样本空间特别稠密，在类别空间得到的误差丢给样本空间之后会造成爆炸性的影响，导致模型梯度爆炸，无法收敛。因此，我们需要让类别空间和样本空间之间的分布差异不要太大，也就是它们之间的方差尽可能相等，这就是Xavier初始化的出发点。</p><p>对于 y. = wx ，假设有：<br>$$Var(Y) = Var(w_iX)=Var(w_i)*Var(X)+E^2(X)*Var(w_i)+E^2(w_i)*Var(X)\qquad(1)$$</p><p>当$X, w_i$都符合均值为0的正态分布时，$E(X),E(w_i)$均为0，公式进一步简化为：</p><p>$$Var(Y)=Var(w_i)*Var(X)\qquad(2)$$</p><p>该层共有$n_i$个神经元，则有：</p><p>$$Var(Y)=n_{i}*Var(w_i)*Var(X)\qquad(3)$$</p><p>如果想$Var(x)=Var(Y)$，则需要满足：</p><p>$$n_i*Var(w_i)=1\qquad(4)$$</p><p>$$Var(w_i)=\frac{1}{n_i}\qquad(5)$$</p><p>考虑到反向传播时$n_{i+1}$与$n_i$不相等的情况，取它们的平均：</p><p>$$Var(w_i)=\frac{2}{n_i+n_{i+1}}\qquad(6)$$</p><p>至此，我们得到了使得输入与输出方差相等时参数需要满足的条件。假设参数$w$在$[a,b]$区间内满足均匀分布的话，需要满足：</p><p>$$E=\frac{a+b}{2}=0\qquad(7)$$</p><p>$$Var=\frac{(b-a)^2}{12}=\frac{2}{n_i+n_{i+1}}\qquad(8)$$</p><p>解得Xavier均匀分布：</p><p>$$w \sim U[-\frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}},\frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}}]\qquad(9)$$</p><p>Xavier正态分布：</p><p>$$w \sim N[mean=0, std=\frac{\sqrt{2}}{\sqrt{n_{in}+n_{out}}}]\qquad(10)$$</p><p>Xavier初始化方法存在的缺点：</p><p>Xavier方法的推导过程基于两点假设。</p><p>(1) 激活函数是线性的，并不适应于ReLU，sigmoid等非线性激活函数</p><p>(2) 激活函数是关于0对称的，不适应于ReLU，sigmoid等不是关于0对称的激活函数</p><h3 id="Kaiming初始化"><a href="#Kaiming初始化" class="headerlink" title="Kaiming初始化"></a>Kaiming初始化</h3><p>由于Xavier初始化存在的问题，使得它对于在应用ReLU激活函数的模型中表现不佳，因此需要进行修改。这样的话就需要满足每一层输入向量方差相似，每一层的输出向量对应方差相似。</p><p>由于应用了不是关于0对称的激活函数ReLU，公式(1)中的$E(X)$不再为0，因此公式(2)改为：</p><p>$$Var(Y)=Var(w_i)*Var(X)+E^2(X)*Var(w_i)\qquad(11)$$</p><p>考虑到多个神经元：</p><p>$$Var(Y)=n_i*(Var(w_i)*Var(X)+E^2(X)*Var(w_i))\qquad(12)$$</p><p>进一步化简：</p><p>$$Var(Y)=n_i*(Var(w_i)*(E(X^2)-E^2(X))+E^2(X)*Var(w_i))\qquad(13)$$</p><p>$$Var(Y)=n_i*Var(w_i)*E(X^2)\qquad(14)$$</p><p>考虑到每一层的分布，i表示层标记，更改公式(14)为：</p><p>$$Var(Y_i)=n_i*Var(w_i)*E(X_i^2)\qquad(15)$$</p><p>公式(15)中的$E(X_i^2)=Var(X_i)$只有在$X_i$具有0均值的时候才会成立，但是由于$X_i$是由ReLU函数得来的，所以并不成立，$X_i=max(0, Y_{i-1})$。但是如果我们让$w_{i-1}$关于0是对称的，并且偏差$b_{i-1}$为0，这时$Y_{i-1}$均值就会是0，并且关于0对称，这样的话我们就会有：</p><p>$$E(X_i^2)=\frac{1}{2}Var(Y_{i-1})\qquad(16)$$</p><p>公式(15)就可以修改为：</p><p>$$Var(Y_i)=\frac{1}{2}*n_i*Var(w_i)*Var(Y_{i-1})\qquad(17)$$</p><p>如果要满足$Var(Y_i)=Var(Y_{i-1})$，则需要：</p><p>$$\frac{1}{2}n_i*Var(w_i)=1\qquad(18)$$</p><p>$$Var(w_i)=\frac{2}{n_i}\qquad(19)$$</p><p>最终得到前向传播初始化方法:</p><p>$$w \sim N[mean=0, std=\frac{\sqrt{2}}{\sqrt{n_{in}}}]\qquad(20)$$</p><p>反向传播初始化方法：</p><p>$$w \sim N[mean=0, std=\frac{\sqrt{2}}{\sqrt{n_{out}}}]\qquad(21)$$</p><p><strong>参考资料</strong><br>1.<a href="https://zhuanlan.zhihu.com/p/27919794" target="_blank" rel="noopener">深度前馈网络与Xavier初始化原理</a></p><p>2.<a href="https://zhuanlan.zhihu.com/p/40175178" target="_blank" rel="noopener">关于参数初始化的若干问题以及Xavier、He初始化推导</a></p><p>3.<a href="https://pytorch.org/docs/stable/nn.init.html?highlight=kaiming#" target="_blank" rel="noopener">torch.nn.init</a></p><p>4.<a href="https://cloud.tencent.com/developer/article/1437995" target="_blank" rel="noopener">一文详解深度学习参数初始化(weights initializer)策略</a></p><p>5.<a href="https://www.cnblogs.com/shine-lee/p/11908610.html" target="_blank" rel="noopener">网络权重初始化方法总结（下）：Lecun、Xavier与He Kaiming</a></p><p>6.<a href="https://www.researchgate.net/publication/215616968_Understanding_the_difficulty_of_training_deep_feedforward_neural_networks" target="_blank" rel="noopener">Xavier init</a></p><p>7.<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">Kaiming init</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NMS(非极大值抑制算法)</title>
    <link href="undefined2019/12/25/NMS/"/>
    <url>2019/12/25/NMS/</url>
    
    <content type="html"><![CDATA[<h3 id="什么是NMS算法？"><a href="#什么是NMS算法？" class="headerlink" title="什么是NMS算法？"></a>什么是NMS算法？</h3><p>非极大值抑制NMS(Non-maximum Suppression)算法是目标检测任务中一个不可或缺的模块，目标检测的任务是输入一幅图片，利用不同大小、不同尺寸的边界框找出图片中对应的目标物体。</p><center><img src="/2019/12/25/NMS/od.png" srcset="undefined" height="300"> </center><p>但是在实际的检测过程中，我们的检测算法往往会针对一个目标物体，框出许多边界框。</p><center><img src="/2019/12/25/NMS/multi-od.png" srcset="undefined" height="300"> </center><p>但是我们最终对于每个物体实例，最理想的情况下只需要保留一个最佳的边界框，这就应用到了我们的NMS算法。NMS算法通过选定一个得分(confidence score)最高的边界框，过滤掉(抑制)与其相近的其他边界框，并给出相应框对应的类别。</p><center><img src="/2019/12/25/NMS/NMS.png" srcset="undefined"> </center><h3 id="NMS算法实现"><a href="#NMS算法实现" class="headerlink" title="NMS算法实现"></a>NMS算法实现</h3><blockquote><ol><li>输入：边界框集合$\textbf{B}$，对应的IOU阈值$\textbf{T}$</li><li>新建一个空的集合$\textbf{D}$，用于保存最终的结果</li><li>将$\textbf{B}$中的边界框按照得分由高到低排序</li><li>选取$\textbf{B}$中得分最高的边界框$\textbf{m}$，放到$\textbf{D}$中</li><li>计算$\textbf{B}$中剩余边界框与$\textbf{m}$的IOU值，IOU大于$\textbf{T}$的从$\textbf{B}$中拿出过滤掉</li><li>重复步骤4-5，直到$\textbf{B}$为空</li><li>输出：最终保留的边界框集合$\textbf{D}$</li></ol></blockquote><p>这里我们利用YOLOV3里面的NMS实现来进一步说明NMS算法的具体运行过程。</p><pre><code class="python">def nms(boxes, nms_thresh):      # boxes是我们预测的边界框的集合，nms_thresh对应的NMS算法的阈值    res = []    # 将预测的张量转换为numpys matrix数据格式    for box in boxes:        temp = []        for item in box:            if torch.is_tensor(item):                item = float(item.cpu().data.numpy())            temp.append(item)        res.append(temp)    boxes = res    conf = np.zeros(len(boxes))    for i in range(len(boxes)):        conf[i] = boxes[i][4]    sortIndex = list(reversed(np.argsort(conf)))    out_boxes = []    for i in range(len(sortIndex)):        box_i = boxes[sortIndex[i]]        if box_i[4] &gt; 0:            out_boxes.append(box_i)            for j in range(i+1, len(sortIndex)):                box_j = boxes[sortIndex[j]]                iou = cal_iou(box_i, box_j)                if iou &gt; nms_thresh:                    box_j[4] = 0    # 置信度置为0，就是过滤掉对应的边界框    return np.array(out_boxes)</code></pre><h3 id="NMS算法的缺点"><a href="#NMS算法的缺点" class="headerlink" title="NMS算法的缺点"></a>NMS算法的缺点</h3><blockquote><ol><li><p>NMS算法会将相邻的两个边界框强制归零，如果两个边界框都是ground truth，就会将其中一个置信度相对较低的真实边界框过滤掉。</p><center><img src="/2019/12/25/NMS/soft-nms.png" srcset="undefined" height="300"> </center><p>如图所示，我们对两匹马利用检测算法分别得到了两个边界框，它们的置信度分别是0.95和0.8，红色的边界框与绿色的边界框之间的重合度非常高，这样我们很难设定一个正确的阈值。</p></li><li><p>NMS算法的阈值需要人为设定。这个过程需要根据模型以及数据进行调整，才能够获得一个相对良好的设定，这个过程是十分耗费时间的。</p></li><li><p>NMS算法一般只能在CPU上面运行，运行效率低。</p></li></ol></blockquote><h3 id="NMS算法的改进版本"><a href="#NMS算法的改进版本" class="headerlink" title="NMS算法的改进版本"></a>NMS算法的改进版本</h3><h4 id="Soft-NMS"><a href="#Soft-NMS" class="headerlink" title="Soft-NMS"></a>Soft-NMS</h4><p>Soft-NMS的改进思路很简单，原始的NMS算法在算法实现的第五步时，对于IOU值大于阈值的边界框直接将其置信度得分置为0，可以将其视为是一个hard的NMS版本，而Soft-NMS并不是直接将其置为0，而是利用一个置信度衰减函数，降低其置信度得分，这样的话，如果有两个真实边界框，即便是它们之间的IOU值非常高，也不会将其中一个直接过滤掉，而是降低其得分，最终仍然可能被预测为真实值。</p><p>$M$表示当前选择的得分最高的边界框，$b_i$表示剩余的边界框。</p><p>传统的NMS算法置信度更新策略：</p><p>$$ s_i=\begin{cases} s_i &amp; iou(M, b_i)&lt;N_t  \  0 &amp;  iou(M, b_i)\ge N_t \end{cases} $$</p><p>Soft-NMS置信度衰减函数1：</p><p>$$s_i=\begin{cases}s_i &amp;iou(M, b_i)&lt;N_t \ s_i(1-iou(M, b_i)) &amp; iou(M, b_i)\ge N_t \end{cases}$$</p><p>Soft-NMS置信度衰减函数2:</p><p>$$s_i = s_ie^{-\frac{iou(M, b_i)^2}{\sigma}}$$</p><p>第一个衰减函数是非连续的，第二个版本的是连续的。连续函数可以避免一些突变情况的发生。</p><h4 id="Adaptive-NMS"><a href="#Adaptive-NMS" class="headerlink" title="Adaptive-NMS"></a>Adaptive-NMS</h4><p>Adaptive-NMS算法对行人检测领域进行了探讨，行人检测领域两个ground truth更加容易出现overlap的情况，对Soft-NMS又进行了优化，出发点是根据图像人群的不同密度，人群密集的地方NMS算法应该设置一个较大的阈值，在阈值稀疏的地方NMS的阈值就应该较小。</p><center><img src="/2019/12/25/NMS/adaptive-nms.png" srcset="undefined" height="300"> </center><p>作者利用一个CNN网络，针对每个边界框，我们能够利用网络求出边界框位置对应的密度$d_M$，$N_t$仍然是我们预先设定好的阈值，阈值$N_M$取$N_t$和$d_M$的最大值，如果IOU值小于$N_M$，就是传统的NMS，而如果IOU值大于$N_M$，就利用Adaptive NMS的衰减函数来更新置信度得分。</p><p>$$N_M:= max(N_t, d_m)$$</p><p>$$s_i=\begin{cases}s_i, &amp;iou(M, b_i)&lt;N_M \ s_if(1-iou(M, b_i)), &amp; iou(M, b_i)\ge N_M \end{cases}$$</p><p><strong>参考资料</strong><br>1.<a href="https://zhuanlan.zhihu.com/p/70771042" target="_blank" rel="noopener">Detection基础模块之（三）NMS及变体</a></p><p>2.<a href="https://www.zhihu.com/question/279810883/answer/762880189" target="_blank" rel="noopener">目标检测领域NMS（非极大值抑制）的改进算法有哪些？</a></p><p>3.<a href="https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c" target="_blank" rel="noopener">Non-maximum Suppression (NMS)</a></p><p>4.<a href="https://blog.csdn.net/qq_37014750/article/details/89222334" target="_blank" rel="noopener">文阅读：Adaptive NMS: Refining Pedestrian Detection in a Crowd</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>object detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>贝叶斯优化</title>
    <link href="undefined2019/12/10/bayes-optim/"/>
    <url>2019/12/10/bayes-optim/</url>
    
    <content type="html"><![CDATA[<p>深度学习或者机器学习中一个老生常谈的问题就是调参，许多机器学习算法工程师经常戏称自己为“炼丹工程师”，调参过程也就变成了“炼丹”。何谓炼丹？如何调配出一副“仙药”只能靠尝试不同的配方，根据最终的效果(会不会吃死人)来进行改进。</p><p>然后大家渐渐感觉这种炼丹方法不太行。太上老君这种老头为什么能炼金丹？主要是他活得太久，尝试的错误太多，经验也就渐渐积累起来了，放到算法工程师身上情况同样适应，经验丰富的调参手往往一发入魂，新手就是摸着石头过河，难以估计深浅。所以，大家都开始渐渐地意识到自动化调参的必要性，对于个人来说，可以节省大量的时间，去做更多更加有意义的事情(学习)。对公司来说，如果能够实现自动化超参数调整的话，就不必雇佣经验丰富的算法工程师来做相关工作，大大节省了人力成本。</p><p>目前来讲，主流的自动调参算法有Grid Search(网格搜索)、Random Search(随机搜索)、Bayesian Optimization(贝叶斯优化)。随机搜索是最简单的调参算法，但是由于是随机的，往往效果不稳定，波动比较大。网格搜索需要人为定义搜索空间，并且对所有参数进行组合，仍然依赖经验，搜索的时间成本也高，甚至可能出现组合爆炸的现象。贝叶斯优化算法可以采用比较少的观测结果就能够得到一组比较好的超参数组合，因此，贝叶斯优化是目前最好的超参数调节算法(猜的)。</p><h4 id="贝叶斯优化适应的场景"><a href="#贝叶斯优化适应的场景" class="headerlink" title="贝叶斯优化适应的场景"></a>贝叶斯优化适应的场景</h4><blockquote><p>1.模型训练的时间和计算成本非常高。神经网络动辄几百MB的模型参数量。<br>2.优化的目标函数没有导数信息。非凸问题无法利用求导优化。</p></blockquote><h4 id="贝叶斯优化算法"><a href="#贝叶斯优化算法" class="headerlink" title="贝叶斯优化算法"></a>贝叶斯优化算法</h4><p>首先，我们需要定义我们的优化目标。假设我们需要优化的函数f(x)，我们的优化目标就是</p><p>$$x*=argmax \space f(x)$$</p><p>贝叶斯优化是一个序列优化问题。假设我们已经获得了t-1个贝叶斯优化的iteration，我们就获得了一组观察序列 $D_{t-1}$={$(x_1, y_1), (x_2, y_2), (x_3, y_3)…(x_{t-1}, y_{t-1})$} ，这里的x代表的就是我们模型的参数(可以理解为神经网络中的超参)。然后，我们需要根据高斯过程计算前t-1个点的后验分布概率选择第t组超参数$x_t$，不断迭代直到找到最优解。</p><p>那么我们是根据什么去选择这个$x_t$的呢？这也是贝叶斯优化中一个核心问题：如何在每一个iteration里面选择最佳的$x_t$。这就引出了acquisition function的概念。</p><h5 id="acquisition-function"><a href="#acquisition-function" class="headerlink" title="acquisition function"></a>acquisition function</h5><p>我们用函数$\alpha(x)$表示acquisition function:  $$x_t = arg max  \space \alpha_t(x)$$</p><p>我们在设计acquisition function的时候最重要的一点就是要考虑exploration-exploitation tradeoff：我们在选择下一个点$x_t$的时候，既要去尝试我们之前没有尝试的点(exploration)，又要使得取得的点的f(x)的值尽可能大(exploitation)，我们可以分别利用高斯过程的posterior mean和posterior variance来表示exploitation和exploration。mean越大，越符合我们的优化目标，代表我们模型的效果越好，exploitation越大。variance越大，代表我们函数的uncertainty越大，我们就更加可能达到最优值。</p><h4 id="贝叶斯优化的实例"><a href="#贝叶斯优化的实例" class="headerlink" title="贝叶斯优化的实例"></a>贝叶斯优化的实例</h4><p>1.在只有三个初始样本的情况下我们计算出每个点的均值和方差</p><center><img src="/2019/12/10/bayes-optim/pic1.png" srcset="undefined"></center><p>图片中红色的点表示我们实际的观测值，蓝色的线表示真实数据分布，黑色虚线表示我们利用三个点和高斯过程得出的预测结果(后验概率分布)。可以看到中间的点均值和方差都相对来说比较大，也就对应了我们之前提到的exploration和exploitation之间的一个tradeoff。如何权衡exploration和exploitation取决于我们还要进行多少次iteration，如果我们只能再进行1个iteration，那就应该关注exploitation，选择均值大的点。如果我们还有1000次iteration可以进行，就可以关注exploration，去探索更多的参数，寻求全局最优的点。</p><p>2.接下来在原来的图下面添加acquisition function函数曲线，acquisiton function有很多种，最简单的就是posterior mean和posterior variance之间的一个加权和，也就是<a href="https://arxiv.org/abs/0912.3995" target="_blank" rel="noopener">GP-UCB</a>方法，它的acquisition function形式是:</p><center><img src="/2019/12/10/bayes-optim/equation.svg" srcset="undefined"></center><p>选择使得acquisition function值最大的对应参数，利用这个点对应的参数来训练模型，得到模型对应的f(x)值，样本数变成了4个。</p><center><img src="/2019/12/10/bayes-optim/pic2.png" srcset="undefined"></center><p>3.重新计算超参数之间的后验概率分布和acquisition function</p><center><img src="/2019/12/10/bayes-optim/pic3.png" srcset="undefined"></center><p>4.通过不断重复上述步骤，我们能够在样本很少的情况下模拟数据的真实分布(图中的黑色虚线)</p><center><img src="/2019/12/10/bayes-optim/pic4.png" srcset="undefined"></center><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>本文分析了超参数优化中的贝叶斯方法，从贝叶斯方法的基本概念到具体流程进行了详细的阐述，贝叶斯优化方法中的高斯过程本文没有详细阐述，但是本文最后的链接中会有相关链接，感兴趣的可以进行延伸阅读。</p><h4 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h4><p>为什么没有一种调参方法被大规模采用？</p><p>1.最主要的原因还是时间成本太高，包括最近比较火的NAS，动辄几百个小时的训练时间，虽然实现了自动化，但是这样效率真的提高了吗？</p><p>2.即便是实现了自动化调参，调出来的参数效果也不一定比人工经验设定的参数效果好，综合成本是否真的降低了？</p><p><strong>参考资料</strong><br>1.<a href="https://zhuanlan.zhihu.com/p/76269142" target="_blank" rel="noopener">贝叶斯优化/Bayesian Optimization</a><br>2.<a href="https://zhuanlan.zhihu.com/p/29779000" target="_blank" rel="noopener">贝叶斯优化: 一种更好的超参数调优方式</a><br>3.<a href="https://krasserm.github.io/2018/03/19/gaussian-processes/" target="_blank" rel="noopener">Gaussian processes</a><br>4.<a href="https://www.jiqizhixin.com/articles/2019-02-12-3" target="_blank" rel="noopener">看得见的高斯过程：这是一份直观的入门解读</a><br>5.<a href="https://arxiv.org/abs/0912.3995" target="_blank" rel="noopener">Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design</a><br>6.<a href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization" target="_blank" rel="noopener">Practical Bayesian Optimization of Machine Learning Algorithms</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>两阶段目标检测算法</title>
    <link href="undefined2019/11/30/two-stage-detection/"/>
    <url>2019/11/30/two-stage-detection/</url>
    
    <content type="html"><![CDATA[<p>在目标检测任务中，我们可以根据是否产生proposal将模型分为两个类别：单阶段(one-stage)目标检测算法和两阶段(two-stage)目标检测算法。</p><p>本文将整理介绍几种经典的两阶段目标检测算法(R-CNN, SPP Net, Fast R-CNN, Faster R-CNN)，主要从解决的问题、创新点、算法具体流程、仍存在的问题等方面进行一一阐述。</p><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">R-CNN</a></h3><h4 id="时间：2014"><a href="#时间：2014" class="headerlink" title="时间：2014"></a>时间：2014</h4><center><img src="/2019/11/30/two-stage-detection/RCNN.png" srcset="undefined"></center><h4 id="解决的问题："><a href="#解决的问题：" class="headerlink" title="解决的问题："></a>解决的问题：</h4><blockquote><p>如何将深度学习应用于目标检测领域？</p></blockquote><h4 id="创新点："><a href="#创新点：" class="headerlink" title="创新点："></a>创新点：</h4><blockquote><p>首次将深度学习应用到目标检测领域，打破了之前几年时间内传统方法无法获得提升的困境，大大提高了目标检测算法的性能</p></blockquote><h4 id="算法流程："><a href="#算法流程：" class="headerlink" title="算法流程："></a>算法流程：</h4><blockquote><p>1.输入图片<br>2.对输入图片利用selective search算法产生大约2k个region proposal<br>3.对所有的proposal进行warp操作固定大小，然后输入到预训练好的网络中获取feature<br>4.利用线性SVM进行分类</p></blockquote><h4 id="存在的问题："><a href="#存在的问题：" class="headerlink" title="存在的问题："></a>存在的问题：</h4><blockquote><p>1.feature extraction阶段针对每一个proposal都需要通过神经网络计算对应的feature，计算量大<br>2.region proposal、feature extraction、SVM classification都是相互独立的，逐个进行优化，很难得到一个全局最优的结果且效率低<br>3.selective search应用在原始图片上面，获取的内容都是一些底层的信息，很难提取到有效的高层语义信息，而且是在cpu上运行的，比较耗时</p></blockquote><h3 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP Net"></a><a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="noopener">SPP Net</a></h3><h4 id="时间：2014-1"><a href="#时间：2014-1" class="headerlink" title="时间：2014"></a>时间：2014</h4><center><img src="/2019/11/30/two-stage-detection/SPP.png" srcset="undefined"></center><h4 id="解决的问题：-1"><a href="#解决的问题：-1" class="headerlink" title="解决的问题："></a>解决的问题：</h4><blockquote><p>1.卷积神经网络需要输入图片具有相同的尺寸，这就需要对原始的输入图片进行crop/warp操作，这样势必会丢失许多图像信息<br>2.对每个proposal应用神经网络导致大量的计算负担</p></blockquote><center><img src="/2019/11/30/two-stage-detection/crop-wrap.png" srcset="undefined"></center><h4 id="创新点：-1"><a href="#创新点：-1" class="headerlink" title="创新点："></a>创新点：</h4><blockquote><p>1.提出了Spatial Pyramid Pooling，无论针对什么大小的proposal，都能够通过SPP将其映射为固定数目的参数向量，这样就不需要将图片resize到固定大小，也就一定程度上避免了信息的丢失</p></blockquote><blockquote><p>2.针对R-CNN存在的proposal计算特征运算量大的问题，改进为首先对原图像进行卷积操作提取feature map，然后在feature map上面找到proposal对应的的位置，这样就只需要运行一次卷积操作即可，节省了运算时间</p></blockquote><h4 id="算法流程：-1"><a href="#算法流程：-1" class="headerlink" title="算法流程："></a>算法流程：</h4><blockquote><p>1.输入图片<br>2.对输入图片利用selective search算法产生大约2k个region proposal<br>3.对整张输入图片利用卷积神经网络提取feature map<br>4.计算每个proposal在feature map上面的对应位置<br>5.利用Spatial Pyramid Network将proposal对应的feature映射为固定大小<br>6.利用线性SVM进行分类</p></blockquote><h4 id="存在的问题：-1"><a href="#存在的问题：-1" class="headerlink" title="存在的问题："></a>存在的问题：</h4><blockquote><p>1.网络模型仍然是多阶段独立的，无法实现端到端训练<br>2.SPP层之前的网络都是固定权重的，无法进行训练<br>3.proposal生成仍然采用的是selective search的方法</p></blockquote><h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><blockquote><p>SPP Net在训练的时候采用了多尺度的方法，具体流程是：在每一个epoch的时候，需要将图片resize到一个固定的尺寸(224x224)，训练网络后保存网络的参数，下一个epoch开始的时候，将图片resize为另外一个尺寸(180x180)，然后加载之前保存的网络模型参数再次进行训练。</p></blockquote><h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a><a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a></h3><h4 id="时间：2015"><a href="#时间：2015" class="headerlink" title="时间：2015"></a>时间：2015</h4><center><img src="/2019/11/30/two-stage-detection/fast-RCNN.png" srcset="undefined"></center><h4 id="解决的问题：-2"><a href="#解决的问题：-2" class="headerlink" title="解决的问题："></a>解决的问题：</h4><blockquote><p>1.SPP训练过程中对权重微调存在的困难(SPP Net中训练的时候，首先提取所有图片的region proposal，作为训练样本放入到神经网络中进行训练，每个proposal都是来自不同图片的，如果需要对SPP之前的网络进行微调，就需要保存所有的图片的feature map，对计算量和缓存的要求太大)<br>2.之前提出的目标检测算法无法实现端到端训练的过程</p></blockquote><h4 id="创新点：-2"><a href="#创新点：-2" class="headerlink" title="创新点："></a>创新点：</h4><blockquote><p>1.为了解决第一个问题，Fast R-CNN中采用了层次采样的方法。首先从所有样本中每次取N张图片，然后每张图片选取R个ROI，这里的N=2，R=128，同一图像的ROI共享计算和内存，这就意味着每次只需要计算和存储两张图片，资源消耗大大减小</p></blockquote><blockquote><p>2.提出ROI Pooling(Region Of Interest Pooling)。其实是SPP的单尺度版本，论文中提到多尺度能够获得性能提升，但是计算量成倍增加，所以综合考虑之下，单尺度版本的性能更好</p></blockquote><blockquote><p>3.除了selective search提取region proposal过程之外，其他过程(feature extraction、classification、bounding box regression)实现了端到端训练</p></blockquote><blockquote><p>4.将分类和边界框回归整合到一个网络中。不再使用SVM进行分类(通过作者的实验发现，SVM相对于softmax优势不大)，而是利用CNN网络实现了softmax classifier和bounding box regressor同时训练的过程</p></blockquote><h4 id="算法流程：-2"><a href="#算法流程：-2" class="headerlink" title="算法流程："></a>算法流程：</h4><blockquote><p>1.输入图片<br>2.对输入图片利用selective search算法产生大约2k个region proposal<br>3.将预训练网络的max pooling替换为ROI pooling，将每个region proposal映射为固定长度的特征向量<br>4.经过一系列的fc layer之后分为两个branch，一个负责预测proposal的类别，另一个负责进行bounding box regression</p></blockquote><h4 id="存在的问题：-2"><a href="#存在的问题：-2" class="headerlink" title="存在的问题："></a>存在的问题：</h4><blockquote><p>regions proposal仍然采用的是selective search的方法，时间消耗大</p></blockquote><h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a><a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a></h3><h4 id="时间：2016"><a href="#时间：2016" class="headerlink" title="时间：2016"></a>时间：2016</h4><center><img src="/2019/11/30/two-stage-detection/faster-RCNN.png" srcset="undefined"></center><h4 id="解决的问题：-3"><a href="#解决的问题：-3" class="headerlink" title="解决的问题："></a>解决的问题：</h4><blockquote><p>Fast R-CNN虽然基本实现了端到端的训练，但是selective search过程仍然是在cpu上面运行的</p></blockquote><h4 id="创新点：-3"><a href="#创新点：-3" class="headerlink" title="创新点："></a>创新点：</h4><blockquote><p>1.提出了RPN(Region Proposal Network)网络，使得原来的region proposal的过程不再是通过selective的方法，而是通过神经网络来生成，这样region proposal、feature extraction、bounding box regression、classification就融合到一个网络中国呢，真正实现端到端的训练<br>2.第一次引入了anchor的概念，详情可见上一篇文章</p></blockquote><h4 id="算法流程：-3"><a href="#算法流程：-3" class="headerlink" title="算法流程："></a>算法流程：</h4><blockquote><p>1.利用ImageNet模型初始化，训练一个RPN网络<br>2.仍然使用ImageNet模型初始化，利用步骤1产生的proposal作为输入，训练一个Fast R-CNN网络。至此，权值不共享<br>3.利用步骤2训练好的Fast R-CNN初始化一个新的RPN网络(就是前几层的网络权重设置为同样的值)，将RPN和Fast R-CNN共享的权值learning rate设置为0，不更新，只更新RPN特有的网络层，至此，两个网络就共享了所有的卷积层<br>4.仍然固定共享的卷积层，将Fast R-CNN加进来，fine-tune Fast R-CNN对应的网络层</p></blockquote><p><strong>参考资料</strong></p><p>1.<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">R-CNN</a><br>2.<a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="noopener">SPP Net</a><br>3.<a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a><br>4.<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a><br>5.<a href="https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html" target="_blank" rel="noopener">Object Detection for Dummies Part 3: R-CNN Family</a><br>6.<a href="https://www.zhihu.com/question/35887527/answer/147832196" target="_blank" rel="noopener">如何评价rcnn、fast-rcnn和faster-rcnn这一系列方法？</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>object detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>目标检测中的Anchor</title>
    <link href="undefined2019/11/21/anchor/"/>
    <url>2019/11/21/anchor/</url>
    
    <content type="html"><![CDATA[<h1 id="什么是Anchor？"><a href="#什么是Anchor？" class="headerlink" title="什么是Anchor？"></a>什么是Anchor？</h1><p>anchor机制是广泛应用于目标检测任务中的一种方法，anchor box指的是是一组预先定义好的不同长宽比的边界框，通过对这些不同长宽比的anchor进行微调，我们能够检测到对应不同尺寸的真实物体。</p><center><img src="/2019/11/21/anchor/anchor.png" srcset="undefined" width="200" height="200" alt="目标检测中的anchor">目标检测中的anchor</center><h1 id="Anchor的作用是什么？"><a href="#Anchor的作用是什么？" class="headerlink" title="Anchor的作用是什么？"></a>Anchor的作用是什么？</h1><p>考虑到图像中会出现不同尺寸(scale)，不同长宽比(aspect ratio)的物体，而我们神经网络的输出是一组同样scale的feature map和一组同样的weights，这样的情况下让其预测不同scale和aspect ratio的物体就相对来说比较困难。如图所示：</p><center><img src="/2019/11/21/anchor/multiscale.jpg" srcset="undefined">图像中的多尺度物体</center><p><a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" target="_blank" rel="noopener">Faster-RCNN</a>中首先引入anchor的概念，论文中有这样一段话:</p><blockquote><p>In contrast to prevalent methods [8], [9], [1], [2] that use pyramids of images (Figure 1, a) or pyramids of filters (Figure 1, b), we introduce novel “anchor” boxes that serve as <strong>references at multiple scales and aspect ratios</strong>. Our scheme can be thought of as a pyramid of regression references (Figure 1, c), which avoids enumerating images or filters of multiple scales or aspect ratios. This model performs well when trained and tested using single-scale images and thus benefits running speed.</p></blockquote><p>也就是说，anchor的作用在于解决图像中物体scale和aspect ratio变化范围大的问题，之前的解决方法是pyramids of images 或者 pyramids of filters，但是前者耗费的时间太多，而后者是传统的CV的方法，当时对于CNN还没有一种一种对应的pyramids of filters的方案(真正得到CNN的方法是2016年提出的FPN，pyramids of features)</p><p>anchor的方法同时还解决了另外一个问题。我们在做检测的时候，如果两个ground truth预测框之间的overlap过大，它们会落到同一个cell中，这样就会导致ground truth被过滤掉。如下图所示：</p><center><img src="/2019/11/21/anchor/overlap.jpeg" srcset="undefined">同一个cell中gt box出现overlap的情况</center><p>而如果采用的anchor方法，一个cell中不同的anchor负责去预测不同scale和aspect ratio的物体，即便是有多个框会预测到同一个cell，也不会导致ground truth丢失的情况出现。</p><p><strong>总结来说，anchor机制解决了两个问题：(1) 不同scale和aspect ratio物体预测难度大，利用anchor将scale和aspect ratio空间划分为几个子空间，降低了问题难度  (2) 解决了gt box与gt box之间overlap过大导致gt box丢失的问题。</strong></p><h1 id="Anchor的选取"><a href="#Anchor的选取" class="headerlink" title="Anchor的选取"></a>Anchor的选取</h1><p>anchor的大小是在我们进行模型训练之前就已经确定的了，那么是如何确定的呢？  </p><p>我们以Pascal VOC 2007数据集为例，采用的方法是k-means聚类，根据VOC 2007数据集中图片的长度和宽度进行聚类，这里需要注意的问题是，我们采用的是Yolov3模型，Yolov3模型采用的输入图像的size 是416*416的，因此在做聚类之前就需要将图片和对应的label做等比例的resize，然后再进行聚类，获取anchor。  </p><p>聚类代码：</p><pre><code class="python">class kMean_parse:    def __init__(self,path_txt):        self.path = path_txt        self.km = KMeans(n_clusters=9,init=&quot;k-means++&quot;,n_init=10,max_iter=3000000,tol=1e-3,random_state=0)        self._load_data()    def _load_data (self):        self.data = np.loadtxt(self.path)    def parse_data (self):        self.y_k = self.km.fit_predict(self.data)        print(self.km.cluster_centers_)    def plot_data (self):        plt.scatter(self.data[self.y_k == 0, 0], self.data[self.y_k == 0, 1], s=50, c=&quot;orange&quot;, marker=&quot;o&quot;, label=&quot;cluster 1&quot;)        plt.scatter(self.data[self.y_k == 1, 0], self.data[self.y_k == 1, 1], s=50, c=&quot;green&quot;, marker=&quot;s&quot;, label=&quot;cluster 2&quot;)        plt.scatter(self.data[self.y_k == 2, 0], self.data[self.y_k == 2, 1], s=50, c=&quot;blue&quot;, marker=&quot;^&quot;, label=&quot;cluster 3&quot;)        plt.scatter(self.data[self.y_k == 3, 0], self.data[self.y_k == 3, 1], s=50, c=&quot;gray&quot;, marker=&quot;*&quot;,label=&quot;cluster 4&quot;)        plt.scatter(self.data[self.y_k == 4, 0], self.data[self.y_k == 4, 1], s=50, c=&quot;yellow&quot;, marker=&quot;d&quot;,label=&quot;cluster 5&quot;)       # draw the centers        plt.scatter(self.km.cluster_centers_[:, 0], self.km.cluster_centers_[:, 1], s=250, marker=&quot;*&quot;, c=&quot;red&quot;, label=&quot;cluster center&quot;)        plt.legend()        plt.grid()        plt.show()</code></pre><p>完整的代码可以参考<a href="https://github.com/cowarder/yolov3_voc/blob/master/k-means.py" target="_blank" rel="noopener">这里</a></p><p><strong>参考资料</strong></p><p>1.<a href="https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html" target="_blank" rel="noopener">Anchor Boxes for Object Detection</a><br>2.<a href="https://towardsdatascience.com/neural-networks-intuitions-5-anchors-and-object-detection-fc9b12120830" target="_blank" rel="noopener">Neural Networks Intuitions: 5. Anchors and Object Detection</a><br>3.<a href="https://zhuanlan.zhihu.com/p/49995236" target="_blank" rel="noopener">史上最详细的Yolov3边框预测分析</a><br>4.<a href="https://zhuanlan.zhihu.com/p/73024408" target="_blank" rel="noopener">Why anchor?</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>object detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>翻译：A Recipe for Training Neural Networks</title>
    <link href="undefined2019/11/04/dl-recipe/"/>
    <url>2019/11/04/dl-recipe/</url>
    
    <content type="html"><![CDATA[<p>原文链接：<a href="http://karpathy.github.io/2019/04/25/recipe/" target="_blank" rel="noopener">A Recipe for Training Neural Networks</a></p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>&emsp; 几周之前我发了一条推特<a href="https://twitter.com/karpathy/status/1013244313327681536?lang=en" target="_blank" rel="noopener">“最常见的神经网络错误”</a>，其中列出了几条在训练神经网络过程中常见的陷阱。这条推特比我预想的得到了更多的关注。很明显，很多人都亲身经历过”我们的网络能够work”与”我们的网络实现了state of the art的效果”之间的巨大性能差异。</p><p>&emsp; 所以我觉得如果将这条推特通过博客的形式来延伸一下应该很有趣。但是我并不打算列举出那些常见的错误，我想更加深层次的探讨一下如何去避免这些错误（或者迅速修正他们）。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="1-熟悉你的数据"><a href="#1-熟悉你的数据" class="headerlink" title="1.熟悉你的数据"></a>1.熟悉你的数据</h4><p>&emsp; 构建神经网络的第一步不是去写代码，而是充分的观察你的数据，这一步至关重要。我通常喜欢花费大量时间（小时为单位）来浏览数据样本，从而了解他们的分布规律并寻找数据中可能存在的模式。幸运的是，人类的大脑是非常擅长做这类工作的。通过浏览样本，可能这次我发现数据中包含重复样本，下一次我就能发现损坏的图像/标签。同时我还会寻找数据中的不平衡和偏差问题。通常，我会按照自己的方法将数据分类，这暗示了我们最终要探索的各种数据中的结构。例如：</p><p>&emsp; 仅仅采用局部特征就已经足够或者还需要全局上下文信息？<br>&emsp; 数据中会有多少种变化并且各自呈现出什么形式？<br>&emsp; 哪些变化是虚假的并通过预处理可以过滤掉？<br>&emsp; 空间位置是否重要或者我们希望将其平均池化？<br>&emsp; 数据中的细节信息有多重要？<br>&emsp; 我们能够将图像下采样到什么程度？<br>&emsp; 噪声标签数据有多少？</p><p>&emsp; 除此之外，由于神经网络是数据集的压缩版本，你可以通过可视化网络预测结果来判断其来源，也就是说，如果网络预测结果与数据中观察到的现象不一致，那么就说明可能存在问题。</p><p>&emsp; 一旦通过对数据的观察有了一定的感觉，就可以根据你的想法通过写一些简单的代码来搜索/过滤/排序数据，并可以可视化他们的分布以及沿任意轴向的离群值。离群值几乎总是能揭露在数据质量或者数据预处理中存在的问题</p><h4 id="2-设立一个端到端的训练-评估框架-构建一个基础的baseline"><a href="#2-设立一个端到端的训练-评估框架-构建一个基础的baseline" class="headerlink" title="2.设立一个端到端的训练/评估框架+构建一个基础的baseline"></a>2.设立一个端到端的训练/评估框架+构建一个基础的baseline</h4><p>&emsp; 既然我们充分了解了我们的数据，那是不是就意味着可以开始利用ASPP、FPN、ResNet等网络来进行训练了呢？答案是“NO“。接下来一步是建立一个完整的训练/评估框架，并通过一系列的实验来确保框架的正确性。在这个阶段，最好是选择一些比较简单的模型，例如线性回归，或者是一个非常小的卷积网络。我们将要训练这个模型、可视化损失值、利用模型作出预测，并且利用明确的假设来进行一系列的消融实验。</p><p><strong>Tips &amp; tricks:</strong></p><ul><li>​    <strong><em>固定随机数种子</em></strong>   一定要采取固定随机数种子的方法，这能够保证两次运行代码能够获得同样的结果，对于实验的复现十分重要</li></ul><ul><li><strong><em>简化</em></strong>   确保不要在这个阶段采取一些不必要的fancy trick。比如关闭数据增强。数据增强是一种正则化策略，但是在这个阶段引入可能会带来一些愚蠢的bug</li></ul><ul><li><strong><em>在评估过程中添加有效数据</em></strong>    当绘制验证阶段的损失值的时候，需要对整个验证数据集进行评估，而不是仅仅针对一个batch数据进行评估。我们追求的是准确性，为了确保我们的方法是明智的，有时就需要放弃部分时间效率</li></ul><ul><li><strong><em>确保loss的初始值是正确的</em></strong>   保证损失变量拥有一个正确的初始值。比如，如果正确的初始化了最后一层，应该在初始化时为softmax设置 -log(1/n_classes)。可以为L2回归，Huber损失等导出同样的默认值</li></ul><ul><li><strong><em>初始化</em></strong>   正确地初始化网络的最后一层。例如当你的回归目标是一些均值为50的值，那就应该将最后的偏差设置为50。如果数据不平衡，例如pos:neg的值为1:10，那就应该在logit上面设置偏差，使得网络预测初始化为0.1。正确设置这些参数将加快收敛速度并且避免了在网络训练的初期网络仅仅学习偏差的问题（hockey stick）</li></ul><ul><li><strong><em>人为基线</em></strong>   监控评价指标而不是损失值，这些指标是人为可解释的并且可检查的(例如准确率)，尽可能评估自己设定的评价指标并在模型之间比较。</li></ul><ul><li><strong><em>输入无关的baseline</em></strong>   训练一个输入无关的baseline（最简单的方法是将所有的输入值设置为0），这样要比实际插入数据而不将数据清零时候的效果要差。本质上是说：你的模型完全学会了从输入数据中提取信息了吗？</li></ul><ul><li><strong><em>在小的batch上面过拟合</em></strong>   在只有少量数据的batch上面过拟合。为了这样做，我们需要增加我们模型的容量（例如添加更多的层数）并确保我们可以获取最低loss值。我也喜欢可视化预测值和真实值，确保最终它们之间能够完美的拟合，从而得到最低的loss值。如果不能够在小batch上面实现过拟合，就无法进行下一阶段</li></ul><ul><li><strong><em>在数据传入网络之前可视化</em></strong>   可视化数据的最佳位置应当是在 output = model(input) 代码段之前。也就是说，如果你想准确可视化网络的输入内容，将原始数据张量和标签编码为可视化数据，这是唯一的可靠来源。我已经记不清这种方法曾多少次节省了我的时间并且揭露了在预处理阶段和数据增强阶段存在的问题</li></ul><ul><li><strong><em>可视化预测动态信息</em></strong>   我喜欢在训练过程中利用固定的测试数据来可视化模型的预测结果。“动态信息”指的是这些预测结果在训练过程中如何变化，这可以在训练过程中给你更好的intuition。很多时候，如果网络以某种方式一直在摆动，这就意味着网络很难去适应你的数据，这其实揭露的是模型的不稳定性。有些时候，过高或者过低的学习率也会造成这种抖动现象</li></ul><ul><li><strong><em>利用backprop传播来绘制依赖关系图</em></strong>   深度学习的代码中经常包含复杂的、向量化的和广播操作。我曾几次遇到的常见bug是，人们会弄错view和permute/transpose之间操作的区别，这样就在无意中搞混了批处理数据的维度。最让人难受的是，即便你这样搞错了，但是你的模型还是能够表现的很好，因为模型自己会学习去忽略这些问题。解决这个问题的方法之一是将loss设置为很小的值，例如将loss设置为样本i输出的总和，然后运行backprop过程达到输入数据，确保只在第i个样本上得到非0的梯度。同样的策略可以应用在模型方面，比如保证t时刻的模型仅仅依赖于1到t-1时刻的模型。更一般的情况下，梯度会提供给你各变量与神经网络依赖关系的信息，这在调试过程中是非常有用的。</li></ul><ul><li><strong><em>泛化特定的代码</em></strong>   这意味着去编写一些更加泛化的代码，但是我经常看见一些人在一开始就尝试写一些“泛化”的代码，这无异于“没有金刚钻，还揽瓷器活”。我喜欢针对我现在在做的事情编写一些更加特殊化的代码，首先应该是使得代码能够跑起来，下一步才是提高它的泛化性能。一般这种情况适应于向量化代码，我总是先会写出一个完整的循环版本，然后针对每个循环逐次将代码向量化。</li></ul><h4 id="3-过拟合"><a href="#3-过拟合" class="headerlink" title="3.过拟合"></a>3.过拟合</h4><p>&emsp; 到现在，我们已经充分了解了我们的数据，并且建立了一个端到端的训练评估系统。给定任意模型，我们能够计算我们相信的度量指标。同时我们还拥有一个输入无关的baseline，几个基础的baseline。接下来就可以迭代一个好的模型了</p><p>&emsp; 我采用的寻找好的模型的方法分为两个阶段：第一个阶段是选取一个足够大的模型确保其能够针对数据做到过拟合，第二个阶段是对模型正则化。这样做的原因是：如果我们的模型不能够实现一个很低的错误率，那就可能预示着其中存在一些问题或者错误的设置。</p><p><strong>Tips &amp; tricks:</strong></p><ul><li><strong><em>选择一个模型</em></strong>   为了达到一个较低的训练损失，应该为给定数据选择一个正确的模型结构。我的第一点建议是：<strong>别逞能</strong>。很多人上来就跟疯了一样像堆乐高似得往网络中加入各种模块，在早期一定要克制这种行为，我建议大家首先查找最相关的文献，然后将文献中最简单的结构复制粘贴到网络中，以实现良好的性能。例如在做图片分类任务的时候，直接复制粘提ResNet-50来运行就可以了</li></ul><ul><li><strong><em>Adam是安全的优化器</em></strong>   在构建baseline的早期阶段，我喜欢采用Adam优化，并且将学习率设置为3e-4。就我的经验来说，Adam对于超参数具有更高的宽容度，包括设置的不好的学习率。对于卷积网络，一个精心调试的SGD达到的性能几乎总是优于Adam，但是最优学习率所在的区域相对更加狭窄（难以调节）并且针对特定的问题会有所变化（注意，如果你用的是RNN结构或者是类似的序列模型的话，更加应该采用RNN）</li></ul><ul><li><strong><em>一次只复杂化一个方面</em></strong>   如果你有多个改进策略要应用到模型当中，我建议你一个一个的插入其中，并且保证每次都能使得模型获得预期的效果提升。一个有效的策略是，首先在较小的图片中实验，然后再应用到较大的图片中。</li></ul><ul><li><strong><em>不要相信学习率衰减默认值</em></strong>   如果你要在现有的任务上复用来自其他任务的代码，一定要小心学习率衰减的设置。不但需要针对不同的问题设置不同的学习率衰减调度策略，调度策略还需要根据当前epoch来调整，因为随着数据量的不同，调度策略变化的范围就会很大。例如，ImageNet每30个epoch降低10倍，如果你不是在训练ImageNet，你一般不会这样设置。如果不小心调整代码，可能学习率就会过早的趋于0，导致模型无法收敛。在我的日常工作中，我一般会将学习率设置为一个常数，不采用学习率衰减，直到最后才会对其进行调整。</li></ul><h4 id="4-正则化"><a href="#4-正则化" class="headerlink" title="4.正则化"></a>4.正则化</h4><p>&emsp; 理想情况下，到目前为止，我们已经有了一个至少在训练数据集上面表现良好的模型。接下来就应该是正则化操作了。通过牺牲一些训练集上面的准确率来获得验证集上准确率的提高。</p><p><strong>Tips &amp; tricks:</strong></p><ul><li><strong><em>获取更多的数据</em></strong>   首先，当前最好并且最流行的正则化网络模型方法就是扩充训练数据。耗费大量的工程周期来从小数据集中压榨模型效果是一个常见的错误，而你原本可以通过收集更多的数据来达到同样的效果。据我所知，增加训练数据几乎是无限提高神经网络性能的唯一保证方法（另外一个是ensemble）</li></ul><ul><li><strong><em>数据增强</em></strong>   尝试一些更加随意的数据增强手段</li></ul><ul><li><strong><em>预训练</em></strong>   如果你有足够多的数据，预训练一般不会使你的模型效果更差</li></ul><ul><li><strong><em>坚持使用监督学习</em></strong>   不要对无监督学习保太大期望。据我所知，无监督学习在现代计算机视觉任务上面并没有取得好的效果（即便是Bert在NLP任务上面取得了好的效果，但是可能只是由于文本的重复性特性和更高的信噪比）</li></ul><ul><li><strong><em>更小的输入维度</em></strong>   删除可能包含虚假信号的特征。如果数据集很小，则添加任何虚假信号都有可能过拟合。同样的，如果底层细节不重要，那么可以尝试输入较小的图像。</li></ul><ul><li><strong><em>更小的模型体积</em></strong>   很多情况下，你可以利用先验知识的限制来降低模型的体积。例如，过去常常在backbone的最上层采用一个全连接网络，但是如今却被简单的平均池化所代替，从而消除了大量的参数。</li></ul><ul><li><strong><em>更小的batch size</em></strong>   由于batch normalization中的归一化，更小的batch size一定程度上对应更强的正则化。因为batch上面的经验均值/标准差更近似于全局的均值/标准差，小的batch会带来更大的抖动</li></ul><ul><li><strong><em>dropout</em></strong>   对卷积采用2维dropout，但是在采用dropout的时候要注意，因为在有batch norm的时候，<a href="https://arxiv.org/abs/1801.05134" target="_blank" rel="noopener">dropout表现的不好</a> </li></ul><ul><li><strong><em>权重衰减</em></strong> 增加权重衰减惩罚</li></ul><ul><li><strong><em>early stopping</em></strong>   通过验证集损失的变化来停止训练，避免过拟合</li></ul><ul><li><strong><em>尝试采用更大的模型</em></strong>   我在最后才提这一点，因为我在实验中确实会发现更大的模型会导致过拟合，但是由于有了early stopping策略，大的模型往往会比小的模型效果好，所以我将其放在early stopping后面</li></ul><p>&emsp; 最后，为了证明你训练的是一个合理的分类器，我喜欢可视化网络第一层的权重，确保在这一层你能够获得一些有意义的图像边缘信息。如果在第一层就显得杂乱无章的话，可能就意味着出现了某些问题。同样，内部激活函数输出内容有时也会展现出来一些奇怪的现象，对应着预示了一些问题。</p><h4 id="5-调整"><a href="#5-调整" class="headerlink" title="5.调整"></a>5.调整</h4><p>&emsp; 现在，你应该为了降低验证集上的损失值，而陷入了探索广阔的模型空间的循环之中。在这一步有几点建议：</p><ul><li><strong><em>随机的网络搜索</em></strong>   当需要调整大量超参数的时候，采用网络搜索能够保证所有设置的覆盖范围，但是一定要记住最好是采用<a href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="noopener">随机网格搜索</a>。直观上来讲，神经网络针对一部分超参数更加敏感。如果参数a更加重要而改变参数b的值没有影响，那么你宁愿进行多次采样而不是在固定点上采样</li></ul><ul><li><strong><em>超参数优化</em></strong>   有很多精美的贝叶斯超参数优化工具，我的一些朋友也反映这些工具很好用</li></ul><h4 id="6-进一步提高模型效果"><a href="#6-进一步提高模型效果" class="headerlink" title="6.进一步提高模型效果"></a>6.进一步提高模型效果</h4><p>&emsp; 即便你已经找到了最好的模型结构，并且设置好了最佳的超参数，你仍可以采用一些其他的trick来提高模型的效果</p><ul><li><strong><em>ensemble</em></strong>   模型融合能够保证在任何任务上面大约能够获得2%的提升。如果你不能承担在测试阶段进行模型融合的计算量，可以尝试<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">知识蒸馏</a></li><li><strong><em>让模型持续训练</em></strong>   当验证集上面的损失值不再降低的时候，人们往往倾向于停止训练。根据我的经验，网络可以保持长期训练。有一次我在休冬假的时候，忘记关掉网络，它就一直训练，当我休完假回来之后，发现它达到了state of the art的效果</li></ul><h3 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h3><p>&emsp; 当你读到这里的时候，你已经具备了成功的所有因素：你对技术、数据集、问题有了很深的理解，你已经建立起了完整的训练/评估结构，并对其准确性抱有很高的信心。并且你已经探索了更加复杂的模型，并通过上述步骤获得了性能上面的提升。现在该去读paper，做实验，创造你自己的SOTA模型了。祝好！</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>梯度累加(Gradient Accumulation)</title>
    <link href="undefined2019/10/29/Gradient-Accumulation/"/>
    <url>2019/10/29/Gradient-Accumulation/</url>
    
    <content type="html"><![CDATA[<p>&emsp;  我们在训练神经网络的时候，超参数batch size的大小会对最终的模型效果产生很大的影响。一定条件下，batch size设置的越大，模型就会越稳定。batch size的值通常设置在 8-32 之间，但是当我们做一些计算量需求大的任务(例如语义分割、GAN等)或者输入图片尺寸太大的时候，我们的batch size往往只能设置为2或者4，否则就会出现 “CUDA OUT OF MEMORY” 的不可抗力报错。  </p><p>&emsp;  贫穷是促进人类进步的阶梯，如何在有限的计算资源的条件下，训练时采用更大的batch size呢？这就是<strong>梯度累加(Gradient Accumulation)</strong>技术了。  </p><p>&emsp; 我们以<a href="https://pytorch.org/" target="_blank" rel="noopener">Pytorch</a>为例，一个神经网络的训练过程通常如下：</p><pre><code class="python">    for i, (inputs, labels) in enumerate(trainloader):        optimizer.zero_grad()                   # 梯度清零        outputs = net(inputs)                   # 正向传播        loss = criterion(outputs, labels)       # 计算损失        loss.backward()                         # 反向传播，计算梯度        optimizer.step()                        # 更新参数        if (i+1) % evaluation_steps == 0:            evaluate_model()</code></pre><p>&emsp; 从代码中可以很清楚地看到神经网络是如何做到训练的：<br>                1.将前一个batch计算之后的网络梯度清零<br>                2.正向传播，将数据传入网络，得到预测结果<br>                3.根据预测结果与label，计算损失值<br>                4.利用损失进行反向传播，计算参数梯度<br>                5.利用计算的参数梯度更新网络参数</p><p>&emsp; 下面来看梯度累加是如何做的：</p><pre><code class="python">    for i, (inputs, labels) in enumerate(trainloader):        outputs = net(inputs)                   # 正向传播        loss = criterion(outputs, labels)       # 计算损失函数        loss = loss / accumulation_steps        # 损失标准化        loss.backward()                         # 反向传播，计算梯度        if (i+1) % accumulation_steps == 0:            optimizer.step()                    # 更新参数            optimizer.zero_grad()               # 梯度清零            if (i+1) % evaluation_steps == 0:                evaluate_model()</code></pre><p>&emsp;<br>                1.正向传播，将数据传入网络，得到预测结果<br>                2.根据预测结果与label，计算损失值<br>                3.利用损失进行反向传播，计算参数梯度<br>                4.<strong>重复1-3，不清空梯度，而是将梯度累加</strong><br>                5.梯度累加达到固定次数之后，更新参数，然后将梯度清零  </p><p>&emsp; <strong>总结来讲，梯度累加就是每计算一个batch的梯度，不进行清零，而是做梯度的累加，当累加到一定的次数之后，再更新网络参数，然后将梯度清零。</strong></p><p>&emsp; 通过这种参数延迟更新的手段，可以实现与采用大batch size相近的效果。在平时的实验过程中，我一般会采用梯度累加技术，大多数情况下，采用梯度累加训练的模型效果，要比采用小batch size训练的模型效果要好很多。</p><p><strong>参考资料</strong></p><p>1.<a href="https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3" target="_blank" rel="noopener">Loss Normalization</a><br>2.<a href="https://discuss.pytorch.org/t/model-zero-grad-or-optimizer-zero-grad/28426" target="_blank" rel="noopener">Model.zero_grad() or optimizer.zero_grad()?</a><br>3.<a href="https://www.kaggle.com/c/understanding_cloud_organization/discussion/105614#latest-643946" target="_blank" rel="noopener">A trick to use bigger batches for training: gradient accumulation</a><br>4.<a href="https://www.zhihu.com/question/303070254/answer/573037166" target="_blank" rel="noopener">PyTorch中在反向传播前为什么要手动将梯度清零？</a><br>5.<a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255" target="_blank" rel="noopener">Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>